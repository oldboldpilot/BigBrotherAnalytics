/**
 * Comprehensive ML Engine Benchmark
 *
 * Compares all inference engines with the 60-parameter price_predictor model:
 * - MKL (Intel MKL BLAS)
 * - SIMD AVX-512 + FMA
 * - SIMD AVX-2 + FMA
 * - ONNX Runtime (baseline)
 *
 * All engines load the same weights using the unified weight_loader module.
 *
 * Author: Olumuyiwa Oluwasanmi
 * Date: 2025-11-13
 */

#include <chrono>
#include <iomanip>
#include <iostream>
#include <vector>
#include <array>
#include <cmath>
#include <algorithm>

// Import C++23 modules
import bigbrother.ml.weight_loader;
import bigbrother.ml.neural_net_mkl;
import bigbrother.ml.neural_net_simd;

using namespace bigbrother::ml;
using namespace std::chrono;

// Benchmark configuration
constexpr int WARMUP_ITERATIONS = 100;
constexpr int BENCHMARK_ITERATIONS = 10000;

struct BenchmarkResult {
    std::string engine_name;
    double mean_latency_us;
    double std_latency_us;
    double min_latency_us;
    double max_latency_us;
    double throughput_per_sec;
    bool weights_loaded;
    std::string instruction_set;
};

/**
 * Generate test input (60 features)
 */
auto generateTestInput() -> std::array<float, 60> {
    std::array<float, 60> input{};
    for (size_t i = 0; i < 60; ++i) {
        input[i] = static_cast<float>(i) / 60.0f;
    }
    return input;
}

/**
 * Compute statistics from timing samples
 */
auto computeStats(std::vector<double> const& samples) -> std::tuple<double, double, double, double> {
    double sum = 0.0;
    double min_val = samples[0];
    double max_val = samples[0];

    for (auto val : samples) {
        sum += val;
        min_val = std::min(min_val, val);
        max_val = std::max(max_val, val);
    }

    double mean = sum / samples.size();

    double variance = 0.0;
    for (auto val : samples) {
        double diff = val - mean;
        variance += diff * diff;
    }
    variance /= samples.size();
    double std_dev = std::sqrt(variance);

    return {mean, std_dev, min_val, max_val};
}

/**
 * Benchmark MKL implementation
 */
auto benchmarkMKL() -> BenchmarkResult {
    BenchmarkResult result;
    result.engine_name = "Intel MKL BLAS";
    result.instruction_set = "MKL Optimized";

    try {
        // Load weights using fluent API
        std::cout << "\n[MKL] Loading weights using fluent API...\n";
        auto weights = PricePredictorConfig::createLoader()
            .verbose(true)
            .load();

        result.weights_loaded = true;
        std::cout << "[MKL] âœ“ Weights loaded successfully\n";
        std::cout << "[MKL]   Total parameters: " << weights.total_params << "\n";
        std::cout << "[MKL]   Layers: " << weights.num_layers << "\n";

        // TODO: Create MKL network and load weights
        // For now, just test the infrastructure
        auto input = generateTestInput();

        // Warmup
        for (int i = 0; i < WARMUP_ITERATIONS; ++i) {
            // Dummy operation
            volatile float sum = 0.0f;
            for (auto v : input) sum += v;
        }

        // Benchmark
        std::vector<double> latencies;
        latencies.reserve(BENCHMARK_ITERATIONS);

        for (int i = 0; i < BENCHMARK_ITERATIONS; ++i) {
            auto start = high_resolution_clock::now();

            // Dummy inference (will be replaced with actual MKL inference)
            volatile float sum = 0.0f;
            for (auto v : input) sum += v;

            auto end = high_resolution_clock::now();
            auto duration_us = duration_cast<microseconds>(end - start).count();
            latencies.push_back(static_cast<double>(duration_us));
        }

        auto [mean, std, min, max] = computeStats(latencies);
        result.mean_latency_us = mean;
        result.std_latency_us = std;
        result.min_latency_us = min;
        result.max_latency_us = max;
        result.throughput_per_sec = 1000000.0 / mean;

    } catch (std::exception const& e) {
        result.weights_loaded = false;
        result.mean_latency_us = 0.0;
        result.throughput_per_sec = 0.0;
        std::cerr << "[MKL] Error: " << e.what() << "\n";
    }

    return result;
}

/**
 * Benchmark SIMD implementation
 */
auto benchmarkSIMD() -> BenchmarkResult {
    BenchmarkResult result;
    result.engine_name = "SIMD Intrinsics";

    try {
        // Detect CPU features
        bool has_avx512 = __builtin_cpu_supports("avx512f");
        bool has_avx2 = __builtin_cpu_supports("avx2");
        bool has_fma = __builtin_cpu_supports("fma");

        if (has_avx512 && has_fma) {
            result.instruction_set = "AVX-512 + FMA";
        } else if (has_avx2 && has_fma) {
            result.instruction_set = "AVX-2 + FMA";
        } else if (has_avx2) {
            result.instruction_set = "AVX-2";
        } else {
            result.instruction_set = "SSE";
        }

        // Load weights using fluent API
        std::cout << "\n[SIMD] Loading weights using fluent API...\n";
        auto weights = WeightLoader::fromDirectory("models/weights")
            .withArchitecture(60, {256, 128, 64, 32}, 3)
            .withNamingScheme("network_{}_weight.bin", "network_{}_bias.bin")
            .verbose(true)
            .load();

        result.weights_loaded = true;
        std::cout << "[SIMD] âœ“ Weights loaded successfully (" << result.instruction_set << ")\n";
        std::cout << "[SIMD]   Total parameters: " << weights.total_params << "\n";

        // TODO: Create SIMD network and load weights
        auto input = generateTestInput();

        // Warmup
        for (int i = 0; i < WARMUP_ITERATIONS; ++i) {
            volatile float sum = 0.0f;
            for (auto v : input) sum += v;
        }

        // Benchmark
        std::vector<double> latencies;
        latencies.reserve(BENCHMARK_ITERATIONS);

        for (int i = 0; i < BENCHMARK_ITERATIONS; ++i) {
            auto start = high_resolution_clock::now();

            volatile float sum = 0.0f;
            for (auto v : input) sum += v;

            auto end = high_resolution_clock::now();
            auto duration_us = duration_cast<microseconds>(end - start).count();
            latencies.push_back(static_cast<double>(duration_us));
        }

        auto [mean, std, min, max] = computeStats(latencies);
        result.mean_latency_us = mean;
        result.std_latency_us = std;
        result.min_latency_us = min;
        result.max_latency_us = max;
        result.throughput_per_sec = 1000000.0 / mean;

    } catch (std::exception const& e) {
        result.weights_loaded = false;
        result.mean_latency_us = 0.0;
        result.throughput_per_sec = 0.0;
        std::cerr << "[SIMD] Error: " << e.what() << "\n";
    }

    return result;
}

/**
 * Print benchmark results table
 */
void printResults(std::vector<BenchmarkResult> const& results) {
    std::cout << "\n";
    std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
    std::cout << "  COMPREHENSIVE ML ENGINE BENCHMARK (60-parameter model)\n";
    std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n";

    std::cout << std::fixed << std::setprecision(2);
    std::cout << std::left;

    std::cout << std::setw(20) << "Engine"
              << std::setw(18) << "Instruction Set"
              << std::setw(12) << "Weights"
              << std::setw(15) << "Mean (Î¼s)"
              << std::setw(15) << "Std (Î¼s)"
              << std::setw(15) << "Throughput"
              << "\n";

    std::cout << "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n";

    for (auto const& r : results) {
        std::cout << std::setw(20) << r.engine_name
                  << std::setw(18) << r.instruction_set
                  << std::setw(12) << (r.weights_loaded ? "âœ“ Loaded" : "âœ— Failed")
                  << std::setw(15) << r.mean_latency_us
                  << std::setw(15) << r.std_latency_us
                  << std::setw(15) << (static_cast<int>(r.throughput_per_sec)) << " /sec"
                  << "\n";
    }

    std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n";

    // Find fastest
    auto fastest = std::min_element(results.begin(), results.end(),
        [](auto const& a, auto const& b) -> bool {
            if (!a.weights_loaded) return false;
            if (!b.weights_loaded) return true;
            return a.mean_latency_us < b.mean_latency_us;
        });

    if (fastest != results.end() && fastest->weights_loaded) {
        std::cout << "ðŸ† Fastest: " << fastest->engine_name
                  << " (" << fastest->instruction_set << ")\n";
        std::cout << "   Latency: " << fastest->mean_latency_us << " Î¼s\n";
        std::cout << "   Throughput: " << static_cast<int>(fastest->throughput_per_sec) << " predictions/sec\n";

        // Compare with others
        std::cout << "\nSpeedup vs others:\n";
        for (auto const& r : results) {
            if (&r != &(*fastest) && r.weights_loaded) {
                double speedup = r.mean_latency_us / fastest->mean_latency_us;
                std::cout << "   vs " << r.engine_name << ": "
                          << std::setprecision(2) << speedup << "x faster\n";
            }
        }
    }

    std::cout << "\n";
}

auto main() -> int {
    std::cout << "BigBrotherAnalytics - ML Engine Benchmark\n";
    std::cout << "Model: price_predictor_best.pth (60 features â†’ 3 outputs)\n";
    std::cout << "Iterations: " << BENCHMARK_ITERATIONS << " (after " << WARMUP_ITERATIONS << " warmup)\n";
    std::cout << "Using C++23 modules with fluent API\n";

    // Verify weights exist using fluent API
    std::cout << "\nVerifying weight files...\n";
    auto loader = PricePredictorConfig::createLoader();
    if (!loader.verify()) {
        std::cerr << "âŒ Weight files verification failed!\n";
        std::cerr << "   Please run: uv run python scripts/ml/export_model_weights_to_cpp.py\n";
        return 1;
    }
    std::cout << "âœ“ All weight files verified\n";

    // Run benchmarks
    std::vector<BenchmarkResult> results;
    results.push_back(benchmarkMKL());
    results.push_back(benchmarkSIMD());

    // Print results
    printResults(results);

    std::cout << "Weight Loader Features:\n";
    std::cout << "  â€¢ C++23 module with fluent API\n";
    std::cout << "  â€¢ Flexible architecture configuration\n";
    std::cout << "  â€¢ Reusable across different neural networks\n";
    std::cout << "  â€¢ Custom naming schemes supported\n";
    std::cout << "  â€¢ Type-safe and zero-cost abstractions\n\n";

    return 0;
}
