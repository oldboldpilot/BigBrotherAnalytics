#!/usr/bin/env python3
"""
Export PyTorch ML model weights to C++ binary format.

This script:
1. Loads the trained PyTorch model
2. Exports all layer weights and biases to binary files
3. Creates a C++ header with weight dimensions and normalization parameters
4. Outputs files that can be directly loaded by Intel MKL-based C++ inference

Author: Olumuyiwa Oluwasanmi
Date: 2025-11-13
"""

import json
import struct
import sys
from pathlib import Path

import numpy as np
import torch


def export_weights_to_binary(model_path: Path, output_dir: Path):
    """Export model weights to binary format for C++."""

    print(f"Loading PyTorch model from {model_path}")

    # Load model checkpoint (allow sklearn.preprocessing for StandardScaler)
    try:
        from sklearn.preprocessing._data import StandardScaler
        torch.serialization.add_safe_globals([StandardScaler])
    except ImportError:
        pass  # sklearn not available, continue anyway

    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    state_dict = checkpoint.get('model_state_dict', checkpoint)

    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)

    # Extract architecture info
    print("\nModel architecture:")
    layer_info = {}

    for key in state_dict.keys():
        if 'weight' in key or 'bias' in key:
            tensor = state_dict[key]
            print(f"  {key}: shape={tensor.shape}, dtype={tensor.dtype}")
            layer_info[key] = {
                'shape': list(tensor.shape),
                'dtype': str(tensor.dtype),
                'size': tensor.numel()
            }

    # Export each layer to binary file
    print("\nExporting weights to binary files...")

    for key, tensor in state_dict.items():
        if 'weight' not in key and 'bias' not in key:
            continue

        # Convert to numpy float32
        weights_np = tensor.cpu().detach().numpy().astype(np.float32)

        # Flatten to 1D array
        weights_flat = weights_np.flatten()

        # Save as binary file (.bin)
        binary_file = output_dir / f"{key.replace('.', '_')}.bin"
        weights_flat.tofile(binary_file)

        print(f"  ✓ {binary_file.name}: {weights_flat.shape[0]} floats ({weights_flat.nbytes} bytes)")

    # Load scaler parameters if available
    features_metadata_path = Path('models/custom_features_metadata.json')
    scaler_mean = None
    scaler_std = None

    if features_metadata_path.exists():
        print(f"\nLoading feature scaler from {features_metadata_path}")
        with open(features_metadata_path, 'r') as f:
            metadata = json.load(f)
            if 'scaler_mean' in metadata and 'scaler_std' in metadata:
                scaler_mean = np.array(metadata['scaler_mean'], dtype=np.float32)
                scaler_std = np.array(metadata['scaler_std'], dtype=np.float32)

                # Save scaler parameters
                scaler_mean_file = output_dir / "scaler_mean.bin"
                scaler_std_file = output_dir / "scaler_std.bin"

                scaler_mean.tofile(scaler_mean_file)
                scaler_std.tofile(scaler_std_file)

                print(f"  ✓ {scaler_mean_file.name}: {len(scaler_mean)} floats")
                print(f"  ✓ {scaler_std_file.name}: {len(scaler_std)} floats")
            else:
                print("  ⚠ Scaler parameters not found in metadata")
    else:
        print(f"  ⚠ Feature metadata not found at {features_metadata_path}")

    # Create C++ header file with weight metadata
    print("\nGenerating C++ header file...")

    header_content = f"""/**
 * Auto-generated model weights metadata
 * Generated by: scripts/ml/export_model_weights_to_cpp.py
 * Source model: {model_path.name}
 * Date: {np.datetime64('now')}
 */

#pragma once

#include <array>
#include <cstdint>
#include <string_view>

namespace bigbrother::ml {{

// Model architecture
struct ModelArchitecture {{
    // Layer dimensions (inferred from weights)
    """

    # Infer layer dimensions from weight shapes
    # Assuming standard feedforward architecture: fc1.weight, fc2.weight, etc.
    layers = {}
    for key in sorted(state_dict.keys()):
        if 'weight' in key:
            tensor = state_dict[key]
            layer_name = key.split('.')[0]  # e.g., "fc1" from "fc1.weight"

            if 'fc' in layer_name or 'linear' in layer_name or 'layer' in layer_name:
                out_features, in_features = tensor.shape
                layers[layer_name] = (in_features, out_features)

    # Write layer dimensions
    for i, (layer_name, (in_dim, out_dim)) in enumerate(sorted(layers.items())):
        header_content += f"    static constexpr int {layer_name}_in = {in_dim};\n"
        header_content += f"    static constexpr int {layer_name}_out = {out_dim};\n"

    header_content += f"""
    // Total parameters
    static constexpr int total_params = {sum(p.numel() for p in state_dict.values())};
}};

// Weight file paths (relative to models/ directory)
struct WeightFiles {{
"""

    # Add file paths for each layer
    for key in sorted(state_dict.keys()):
        if 'weight' in key or 'bias' in key:
            safe_key = key.replace('.', '_')
            header_content += f'    static constexpr std::string_view {safe_key} = "weights/{safe_key}.bin";\n'

    header_content += """};

// Normalization parameters (StandardScaler)
struct NormalizationParams {
"""

    if scaler_mean is not None and scaler_std is not None:
        header_content += f"""    static constexpr int num_features = {len(scaler_mean)};
    static constexpr std::string_view mean_file = "weights/scaler_mean.bin";
    static constexpr std::string_view std_file = "weights/scaler_std.bin";
"""
    else:
        header_content += """    static constexpr int num_features = 60;  // Update this manually
    static constexpr std::string_view mean_file = "";
    static constexpr std::string_view std_file = "";
    // NOTE: Scaler parameters not found! Model may not work correctly.
"""

    header_content += """};

}  // namespace bigbrother::ml
"""

    header_file = output_dir / "model_weights_metadata.hpp"
    with open(header_file, 'w') as f:
        f.write(header_content)

    print(f"  ✓ {header_file.name}")

    # Create summary JSON
    summary = {
        'source_model': str(model_path),
        'export_date': str(np.datetime64('now')),
        'architecture': {
            name: {'in': dims[0], 'out': dims[1]}
            for name, dims in layers.items()
        },
        'layer_info': layer_info,
        'total_parameters': sum(p.numel() for p in state_dict.values()),
        'scaler_available': scaler_mean is not None,
        'num_features': len(scaler_mean) if scaler_mean is not None else None
    }

    summary_file = output_dir / "export_summary.json"
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)

    print(f"  ✓ {summary_file.name}")

    print(f"\n✅ Export complete!")
    print(f"   Output directory: {output_dir}")
    print(f"   Binary files: {len([f for f in output_dir.glob('*.bin')])} files")
    print(f"   Total parameters: {summary['total_parameters']:,}")

    return summary


def main():
    """Main entry point."""

    # Default paths
    model_path = Path("models/custom_price_predictor_best.pth")
    output_dir = Path("models/weights")

    # Allow command-line override
    if len(sys.argv) > 1:
        model_path = Path(sys.argv[1])
    if len(sys.argv) > 2:
        output_dir = Path(sys.argv[2])

    if not model_path.exists():
        print(f"❌ Error: Model file not found: {model_path}")
        print(f"   Usage: {sys.argv[0]} [model_path] [output_dir]")
        sys.exit(1)

    try:
        summary = export_weights_to_binary(model_path, output_dir)

        print("\n" + "="*70)
        print("Next steps:")
        print("="*70)
        print("1. Review the exported weights in models/weights/")
        print("2. Update CMakeLists.txt to include the weights directory")
        print("3. Implement C++ inference using Intel MKL in price_predictor.cppm")
        print("4. Load weights from binary files at initialization")
        print("5. Test inference and compare with PyTorch/ONNX outputs")
        print("="*70)

    except Exception as e:
        print(f"❌ Export failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
