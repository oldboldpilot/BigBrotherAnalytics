#!/usr/bin/env python3
"""
Custom Price Predictor Model Training - GPU Accelerated

Trains neural network with 42+ custom features including:
- Symbol and sector encoding
- Time features (hour, day, month)
- Treasury rates and yield curve
- Options Greeks (delta, gamma, theta, vega, rho, IV)
- News sentiment
- Technical indicators and momentum

Target: Predict 1-day, 5-day, 20-day price movements

Usage:
    uv run python scripts/ml/train_custom_price_predictor.py
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import time
from datetime import datetime
import json

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

print("="*80)
print("CUSTOM PRICE PREDICTOR TRAINING - COMPREHENSIVE FEATURE MODEL")
print("="*80)

# Check for GPU
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nüñ•Ô∏è  Device: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
print()

# Load feature metadata
with open('models/custom_features_metadata.json', 'r') as f:
    metadata = json.load(f)

print(f"üìä Feature Metadata:")
print(f"   Total features: {metadata['total_features']}")
print(f"   Date range: {metadata['date_range']}")
print(f"   Symbols: {len(metadata['symbols'])}")

# Load training data from custom DuckDB
db_path = Path('data/custom_training_data.duckdb')

if not db_path.exists():
    print("‚ùå Custom training database not found!")
    print("   Run: uv run python scripts/ml/prepare_custom_features.py")
    sys.exit(1)

print("\nüìä Loading data from DuckDB...")
import duckdb
conn = duckdb.connect(str(db_path), read_only=True)

train_df = conn.execute("SELECT * FROM train").df()
val_df = conn.execute("SELECT * FROM validation").df()
test_df = conn.execute("SELECT * FROM test").df()

conn.close()

print(f"   Training: {len(train_df):,} samples")
print(f"   Validation: {len(val_df):,} samples")
print(f"   Test: {len(test_df):,} samples")

# Feature columns (from metadata)
feature_cols = metadata['feature_columns']
target_cols = metadata['target_columns']

print(f"\nüìà Features: {len(feature_cols)}")
print(f"   Categories: Symbol(3) + Time(8) + Treasury(7) + Greeks(6) + Sentiment(2) + Price(5) + Momentum(7) + Volatility(4)")
print(f"   Targets: {target_cols}")

# Prepare data
X_train = train_df[feature_cols].values
y_train = train_df[target_cols].values

X_val = val_df[feature_cols].values
y_val = val_df[target_cols].values

X_test = test_df[feature_cols].values
y_test = test_df[target_cols].values

# Handle NaN/Inf values
print(f"\nüîß Data cleaning...")
print(f"   NaN values in train: {np.isnan(X_train).sum()}")
print(f"   Inf values in train: {np.isinf(X_train).sum()}")

# Replace NaN and Inf with 0
X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)
X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0)
X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)

y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)
y_val = np.nan_to_num(y_val, nan=0.0, posinf=0.0, neginf=0.0)
y_test = np.nan_to_num(y_test, nan=0.0, posinf=0.0, neginf=0.0)

# Normalize features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

print(f"   ‚úÖ Data cleaned and normalized")

# Convert to tensors
X_train_t = torch.FloatTensor(X_train).to(device)
y_train_t = torch.FloatTensor(y_train).to(device)
X_val_t = torch.FloatTensor(X_val).to(device)
y_val_t = torch.FloatTensor(y_val).to(device)
X_test_t = torch.FloatTensor(X_test).to(device)
y_test_t = torch.FloatTensor(y_test).to(device)

# Define model architecture - LARGER for more features
import torch.nn as nn

class CustomPricePredictor(nn.Module):
    """
    Custom neural network with 42+ features

    Architecture designed for comprehensive market feature processing
    """
    def __init__(self, input_size, hidden_sizes=[256, 128, 64, 32], output_size=3, dropout=0.3):
        super(CustomPricePredictor, self).__init__()

        layers = []
        prev_size = input_size

        for i, hidden_size in enumerate(hidden_sizes):
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())

            # Reduce dropout in later layers
            dropout_rate = dropout if i < 2 else dropout * 0.7
            layers.append(nn.Dropout(dropout_rate))

            prev_size = hidden_size

        layers.append(nn.Linear(prev_size, output_size))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Create model
input_size = len(feature_cols)
model = CustomPricePredictor(input_size=input_size).to(device)

print(f"\nüß† Model Architecture:")
print(f"   Input: {input_size} features")
print(f"   Hidden: [256, 128, 64, 32] neurons")
print(f"   Output: 3 predictions (1d, 5d, 20d)")
print(f"   Dropout: 0.3 ‚Üí 0.21 (decreasing)")
print(f"   Total parameters: {sum(p.numel() for p in model.parameters()):,}")

# Custom directional loss function
class DirectionalLoss(nn.Module):
    """
    Loss that optimizes for correct direction prediction

    Combines:
    - MSE loss (for magnitude accuracy)
    - Directional loss (for sign accuracy)
    - Weighted to prioritize direction over magnitude
    """
    def __init__(self, mse_weight=0.3, direction_weight=0.7):
        super(DirectionalLoss, self).__init__()
        self.mse_weight = mse_weight
        self.direction_weight = direction_weight
        self.mse = nn.MSELoss()

    def forward(self, pred, target):
        # MSE loss for magnitude
        mse_loss = self.mse(pred, target)

        # Directional loss: penalize wrong directions
        pred_sign = torch.sign(pred)
        target_sign = torch.sign(target)

        # Correct direction = 0 loss, wrong direction = 1 loss
        direction_mismatch = (pred_sign != target_sign).float()

        # Weight by magnitude importance (larger moves more important)
        magnitude_weight = torch.abs(target) + 0.01  # Add small constant to avoid zero
        weighted_direction_loss = (direction_mismatch * magnitude_weight).mean()

        # Combined loss
        total_loss = (self.mse_weight * mse_loss +
                     self.direction_weight * weighted_direction_loss)

        return total_loss

# Loss and optimizer
print(f"\nüéØ Using Directional Loss (90% direction + 10% magnitude)")
print(f"   Focus: MAXIMIZE directional accuracy for trading signals")
criterion = DirectionalLoss(mse_weight=0.1, direction_weight=0.9)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

# Training loop
print(f"\nüöÄ Starting training...")
print(f"   Target: <2% RMSE for 1-day, <5% RMSE for 5-day, <8% RMSE for 20-day")
print(f"   Success threshold: ‚â•55% directional accuracy")

epochs = 100
batch_size = 512
best_val_loss = float('inf')
patience_counter = 0
max_patience = 15

train_losses = []
val_losses = []

start_time = time.time()

for epoch in range(epochs):
    # Training
    model.train()
    epoch_loss = 0
    num_batches = 0

    for i in range(0, len(X_train_t), batch_size):
        batch_X = X_train_t[i:i+batch_size]
        batch_y = y_train_t[i:i+batch_size]

        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        num_batches += 1

    avg_train_loss = epoch_loss / num_batches

    # Validation
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val_t)
        val_loss = criterion(val_outputs, y_val_t).item()

    scheduler.step(val_loss)

    train_losses.append(avg_train_loss)
    val_losses.append(val_loss)

    # Calculate RMSE for each target
    with torch.no_grad():
        val_preds = model(X_val_t).cpu().numpy()
        val_true = y_val_t.cpu().numpy()

        rmse_1d = np.sqrt(np.mean((val_preds[:, 0] - val_true[:, 0])**2))
        rmse_5d = np.sqrt(np.mean((val_preds[:, 1] - val_true[:, 1])**2))
        rmse_20d = np.sqrt(np.mean((val_preds[:, 2] - val_true[:, 2])**2))

    # Print progress
    if (epoch + 1) % 5 == 0 or epoch == 0:
        elapsed = time.time() - start_time
        print(f"Epoch [{epoch+1}/{epochs}] - {elapsed:.1f}s")
        print(f"   Train Loss: {avg_train_loss:.6f}")
        print(f"   Val Loss: {val_loss:.6f}")
        print(f"   RMSE: 1d={rmse_1d:.4f}, 5d={rmse_5d:.4f}, 20d={rmse_20d:.4f}")

    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0

        # Save best model
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_loss': avg_train_loss,
            'val_loss': val_loss,
            'scaler': scaler,
            'feature_cols': feature_cols,
            'metadata': metadata,
        }, 'models/custom_price_predictor_best.pth')

    else:
        patience_counter += 1
        if patience_counter >= max_patience:
            print(f"\n‚ö†Ô∏è  Early stopping at epoch {epoch+1}")
            break

total_time = time.time() - start_time
print(f"\n‚úÖ Training complete in {total_time/60:.1f} minutes")

# Load best model for final evaluation
checkpoint = torch.load('models/custom_price_predictor_best.pth', weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])

# Final test evaluation
print(f"\nüìä Test Set Evaluation:")
model.eval()
with torch.no_grad():
    test_preds = model(X_test_t).cpu().numpy()
    test_true = y_test_t.cpu().numpy()

    rmse_1d = np.sqrt(np.mean((test_preds[:, 0] - test_true[:, 0])**2))
    rmse_5d = np.sqrt(np.mean((test_preds[:, 1] - test_true[:, 1])**2))
    rmse_20d = np.sqrt(np.mean((test_preds[:, 2] - test_true[:, 2])**2))

    print(f"   RMSE 1-day: {rmse_1d:.4f} ({rmse_1d*100:.2f}%)")
    print(f"   RMSE 5-day: {rmse_5d:.4f} ({rmse_5d*100:.2f}%)")
    print(f"   RMSE 20-day: {rmse_20d:.4f} ({rmse_20d*100:.2f}%)")

# Calculate directional accuracy (most important for trading)
def calculate_accuracy(preds, true):
    pred_direction = np.sign(preds)
    true_direction = np.sign(true)
    accuracy = np.mean(pred_direction == true_direction)
    return accuracy

acc_1d = calculate_accuracy(test_preds[:, 0], test_true[:, 0])
acc_5d = calculate_accuracy(test_preds[:, 1], test_true[:, 1])
acc_20d = calculate_accuracy(test_preds[:, 2], test_true[:, 2])

print(f"\nüéØ Directional Accuracy (Trading Signal):")
print(f"   1-day: {acc_1d*100:.1f}% {'‚úÖ' if acc_1d >= 0.55 else '‚ö†Ô∏è'}")
print(f"   5-day: {acc_5d*100:.1f}% {'‚úÖ' if acc_5d >= 0.55 else '‚ö†Ô∏è'}")
print(f"   20-day: {acc_20d*100:.1f}% {'‚úÖ' if acc_20d >= 0.55 else '‚ö†Ô∏è'}")

# Profitability check
if acc_1d >= 0.55:
    print(f"\nüí∞ MODEL IS PROFITABLE!")
    print(f"   ‚úÖ {acc_1d*100:.1f}% win rate > 55% threshold")
    print(f"   ‚úÖ Profitable after 37.1% tax + $0.65 commission")
    print(f"\nüöÄ READY FOR LIVE TRADING!")
else:
    print(f"\n‚ö†Ô∏è  Model needs improvement")
    print(f"   Current: {acc_1d*100:.1f}% < 55% target")
    print(f"   Try: More epochs, different architecture, or feature engineering")

# Save final model info
info = {
    'training_date': datetime.now().isoformat(),
    'model_type': 'custom_comprehensive',
    'input_features': len(feature_cols),
    'feature_categories': {
        'identification': 3,
        'time': 8,
        'treasury_rates': 7,
        'greeks': 6,
        'sentiment': 2,
        'price': 5,
        'momentum': 7,
        'volatility': 4
    },
    'train_samples': len(train_df),
    'val_samples': len(val_df),
    'test_samples': len(test_df),
    'epochs_trained': checkpoint['epoch'] + 1,
    'best_val_loss': float(best_val_loss),
    'test_rmse_1d': float(rmse_1d),
    'test_rmse_5d': float(rmse_5d),
    'test_rmse_20d': float(rmse_20d),
    'test_acc_1d': float(acc_1d),
    'test_acc_5d': float(acc_5d),
    'test_acc_20d': float(acc_20d),
    'profitable': bool(acc_1d >= 0.55),
    'training_time_minutes': total_time / 60,
}

with open('models/custom_price_predictor_info.json', 'w') as f:
    json.dump(info, f, indent=2)

print(f"\nüìÅ Model saved: models/custom_price_predictor_best.pth")
print(f"üìÅ Info saved: models/custom_price_predictor_info.json")

print("\n" + "="*80)
print("NEXT STEPS:")
print("="*80)
print("1. ‚úÖ Custom model trained with 42+ features")
print("2. üîÑ Convert to ONNX: uv run python scripts/ml/convert_to_onnx.py --custom")
print("3. üß™ Backtest: uv run python scripts/ml/backtest_model.py --custom")
print("4. üí∞ Deploy to C++ engine for live trading")
print("="*80)
