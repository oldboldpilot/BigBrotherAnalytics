# Intelligent Trading Decision Engine - Architecture Design Document

**Version:** 1.0.0
**Date:** November 6, 2025
**Status:** Draft - Design Phase
**Author:** Olumuyiwa Oluwasanmi
**Related:** [Product Requirements Document](../PRD.md)

---

## âš ï¸ DATABASE STRATEGY UPDATE (November 6, 2025)

**IMPORTANT:** This document references PostgreSQL in database schemas and architecture diagrams. However, for **Tier 1 POC (Weeks 1-12)**, we are using **DuckDB-First approach**:

- âœ… **Tier 1 POC:** Use DuckDB EXCLUSIVELY (zero setup, instant start, perfect for real-time trading)
- â¸ï¸  **Tier 2 Production:** Add PostgreSQL ONLY after proving profitability (Month 5+, optional)
- ðŸ“‹ **See detailed analysis:** [Database Strategy Analysis](./database-strategy-analysis.md)

**For Tier 1 implementation:**
- Replace all PostgreSQL/TimescaleDB references with DuckDB
- DuckDB handles trades, positions, orders with full ACID compliance
- PostgreSQL schemas shown below are for Tier 2 reference only
- Migration from DuckDB to PostgreSQL: 1-2 days when needed

---

## Table of Contents

1. [Overview](#1-overview)
2. [Mathematical Foundations](#2-mathematical-foundations)
3. [System Architecture](#3-system-architecture)
4. [Component Architecture](#4-component-architecture)
5. [Explainability & Interpretability](#5-explainability--interpretability)
6. [Historical Data Sources](#6-historical-data-sources)
7. [Charting & Visualization](#7-charting--visualization)
8. [Database Schema Design](#8-database-schema-design)
9. [API Specifications](#9-api-specifications)
10. [Technology Stack](#10-technology-stack)
11. [Deployment Architecture](#11-deployment-architecture)
12. [Implementation Guidelines](#12-implementation-guidelines)

---

## 1. Overview

### 1.1 Purpose

The Intelligent Trading Decision Engine integrates insights from the Market Intelligence Engine and Correlation Analysis Tool to make trading decisions with full explainability. It is NOT a black box - every decision is traceable and understandable.

**Core Capabilities:**
- Integrate signals from Market Intelligence and Correlation tools
- Make trading decisions using Reinforcement Learning and Deep Neural Networks
- Explain every decision with human-readable rationale
- Execute options day trading strategies (initial focus)
- Provide real-time signaling for immediate decisions
- Support medium-term and long-term investment decisions
- Visualize decision-making process with interactive charts

### 1.2 Design Principles

```mermaid
mindmap
  root((Trading Decision Engine))
    Signal Integration
      Market Intelligence
      Correlation Data
      Technical Indicators
      Sentiment Scores
    Mathematical Models
      Reinforcement Learning
      Deep Neural Networks
      Graph Neural Networks
      Ensemble Methods
    Decision Making
      Options Strategies
      Position Sizing
      Risk Management
      Entry/Exit Signals
    Explainability
      Feature Importance
      Decision Trees
      Attention Weights
      Human Rationale
    Execution
      Order Routing
      Portfolio Management
      Performance Tracking
      Real-time Alerts
```

### 1.3 Not a Black Box - Explainability First

**CRITICAL:** Every trading decision must be explainable to humans. No "trust the AI" - full transparency.

```mermaid
graph TB
    Signal[Trading Signal] --> Analysis[Signal Analysis]
    Analysis --> Model[ML Model]
    Model --> Decision[Trading Decision]

    Decision --> Explain[Generate Explanation]

    Explain --> Why1[Why this trade?<br/>Feature importance]
    Explain --> Why2[Why this size?<br/>Kelly criterion calc]
    Explain --> Why3[Why this price?<br/>Entry logic]
    Explain --> Why4[Why now?<br/>Timing rationale]

    Why1 & Why2 & Why3 & Why4 --> Human[Human-Readable<br/>Explanation]

    Human --> Audit[Audit Trail]
    Human --> Visual[Visualization]
    Human --> Override[Human Override<br/>Possible]

    style Explain fill:#faa,stroke:#333,stroke-width:3px
    style Human fill:#afa,stroke:#333,stroke-width:2px
```

---

## 2. Mathematical Foundations

### 2.1 Core Mathematical Components

```mermaid
graph TB
    subgraph "Mathematical Framework"
        M1[Linear Algebra<br/>Matrices & Vectors<br/>MKL/cuBLAS]
        M2[Graph Theory<br/>Impact Networks<br/>Apache AGE]
        M3[Probability Theory<br/>Bayesian Inference<br/>PyMC3]
        M4[Optimization<br/>Convex/Non-convex<br/>scipy.optimize]
    end

    subgraph "Machine Learning"
        ML1[Reinforcement Learning<br/>PPO, DQN, A3C<br/>Stable-Baselines3]
        ML2[Deep Neural Networks<br/>Transformers, LSTMs<br/>PyTorch + CUDA]
        ML3[Graph Neural Networks<br/>Impact Propagation<br/>PyG/DGL]
        ML4[Ensemble Methods<br/>Model Averaging<br/>XGBoost/LightGBM]
    end

    subgraph "Decision Theory"
        D1[Expected Value<br/>E[Return]]
        D2[Risk-Adjusted Returns<br/>Sharpe/Sortino]
        D3[Kelly Criterion<br/>Position Sizing]
        D4[Multi-Objective<br/>Return vs Risk]
    end

    M1 & M2 & M3 & M4 --> ML1 & ML2 & ML3 & ML4
    ML1 & ML2 & ML3 & ML4 --> D1 & D2 & D3 & D4

    style ML2 fill:#faa,stroke:#333,stroke-width:2px
    style ML1 fill:#f9f,stroke:#333,stroke-width:2px
    style D3 fill:#bbf,stroke:#333,stroke-width:2px
```

### 2.2 Reinforcement Learning Framework

**Environment:** Market as MDP (Markov Decision Process)

```python
# Mathematical Formulation
State (s_t):
  - Current portfolio positions
  - Market intelligence predictions
  - Correlation matrices
  - Technical indicators
  - Market sentiment
  - Account balance

Action (a_t):
  - Buy (symbol, quantity)
  - Sell (symbol, quantity)
  - Hold
  - Options strategies (call, put, spread)

Reward (r_t):
  - Immediate: Realized P&L
  - Delayed: Risk-adjusted return (Sharpe ratio)
  - Penalty: Drawdown, volatility
  - Bonus: Beating benchmark

Transition: P(s_{t+1} | s_t, a_t)
  - Stochastic market dynamics
  - Order execution uncertainty
```

**RL Algorithms (Explained):**

```mermaid
graph LR
    subgraph "Policy-Based (PPO)"
        P1[Policy Network<br/>Ï€(a|s)]
        P2[Value Network<br/>V(s)]
        P3[Advantage<br/>A = Q - V]
        P4[Update Policy<br/>âˆ‡J(Î¸)]
    end

    subgraph "Value-Based (DQN)"
        V1[Q-Network<br/>Q(s,a)]
        V2[Target Network<br/>QÌ‚(s,a)]
        V3[Experience Replay<br/>Memory]
        V4[Update Q<br/>Bellman]
    end

    subgraph "Actor-Critic (A3C)"
        A1[Actor<br/>Policy Ï€]
        A2[Critic<br/>Value V]
        A3[Async Updates<br/>Multiple Envs]
    end

    P1 --> P3 --> P4
    P2 --> P3
    V1 --> V4
    V2 --> V4
    V3 --> V4
    A1 --> A3
    A2 --> A3

    style P1 fill:#faa,stroke:#333
    style V1 fill:#bbf,stroke:#333
    style A1 fill:#afa,stroke:#333
```

**Explainability:**
- PPO policy gradient shows which features influence decisions
- Q-values explain expected returns for each action
- Attention mechanisms highlight important state components

### 2.3 Deep Neural Network Architectures

```mermaid
graph TB
    subgraph "Input Processing"
        I1[Market Intelligence<br/>Features]
        I2[Correlation<br/>Features]
        I3[Technical<br/>Indicators]
        I4[Sentiment<br/>Scores]
    end

    subgraph "Feature Extraction (CNN + Attention)"
        FE1[1D Conv Layers<br/>Time-series patterns]
        FE2[Multi-Head Attention<br/>Important features]
        FE3[Positional Encoding<br/>Time awareness]
    end

    subgraph "Temporal Modeling (LSTM/GRU)"
        TM1[LSTM Encoder<br/>Sequential dependencies]
        TM2[Bidirectional GRU<br/>Past + Future context]
    end

    subgraph "Graph Neural Network"
        GNN1[Impact Graph<br/>Embedding]
        GNN2[Graph Convolution<br/>Multi-hop]
        GNN3[Graph Attention<br/>Weighted aggregation]
    end

    subgraph "Decision Head"
        DH1[Action Classifier<br/>Buy/Sell/Hold]
        DH2[Quantity Regressor<br/>Position Size]
        DH3[Confidence Estimator<br/>Uncertainty]
    end

    subgraph "Explainability"
        EX1[Attention Weights<br/>Feature importance]
        EX2[SHAP Values<br/>Contribution]
        EX3[Layer Activation<br/>What was learned]
    end

    I1 & I2 & I3 & I4 --> FE1 & FE2
    FE1 & FE2 & FE3 --> TM1 & TM2
    I1 --> GNN1 --> GNN2 --> GNN3
    TM1 & TM2 & GNN3 --> DH1 & DH2 & DH3
    DH1 & DH2 & DH3 --> EX1 & EX2 & EX3

    style FE2 fill:#faa,stroke:#333,stroke-width:2px
    style GNN2 fill:#f9f,stroke:#333,stroke-width:2px
    style EX1 fill:#afa,stroke:#333,stroke-width:2px
```

**Mathematical Notation:**

```
Input: x_t = [market_intel_t, correlation_t, technical_t, sentiment_t]

Feature Extraction:
  h_cnn = Conv1D(x_t)
  h_attn = MultiHeadAttention(h_cnn)

Temporal Modeling:
  h_lstm = LSTM(h_attn, h_{t-1})

Graph Embedding:
  h_graph = GCN(impact_graph, x_t)

Decision:
  action_logits = FC(concat(h_lstm, h_graph))
  quantity = Softplus(FC(concat(h_lstm, h_graph)))
  confidence = Sigmoid(FC(concat(h_lstm, h_graph)))

Explainability:
  attention_weights â†’ Top-K features
  SHAP values â†’ Feature contributions
  Activation maps â†’ What patterns detected
```

### 2.4 Matrix Operations for Portfolio Optimization

**Portfolio Optimization (Markowitz Framework):**

```
Objective: Maximize Risk-Adjusted Return

min  w^T Î£ w - Î» Î¼^T w        (Minimize risk, maximize return)
s.t. Î£ w_i = 1                (Fully invested)
     w_i â‰¥ 0                   (Long-only, optional)
     w_i â‰¤ max_position        (Position limits)

Where:
  w = weight vector [n_assets]
  Î£ = covariance matrix [n_assets Ã— n_assets] (from Correlation Tool)
  Î¼ = expected return vector [n_assets] (from Market Intelligence)
  Î» = risk aversion parameter
```

**Implementation with Intel MKL:**

```cpp
// Solve quadratic program using MKL
// Uses correlation matrix from Correlation Tool
// Uses expected returns from Market Intelligence

auto optimize_portfolio_mkl(
    const std::vector<double>& expected_returns,  // From MI Engine
    const std::vector<std::vector<double>>& correlation_matrix,  // From Corr Tool
    double risk_aversion,
    const std::vector<double>& max_positions
) -> std::vector<double> {

    // Convert to MKL-compatible format
    // Use MKL LAPACK for quadratic programming
    // Returns optimal portfolio weights
}
```

### 2.5 Graph-Based Decision Making

**Impact Graph â†’ Trading Decision:**

```mermaid
graph TB
    Event[News Event:<br/>FDA Approves Drug] --> Impact1[Direct Impact<br/>Pharma Company +15%]

    Impact1 --> Impact2[Supplier Impact<br/>Equipment Maker +5%]
    Impact1 --> Impact3[Competitor Impact<br/>Rival Pharma -3%]
    Impact1 --> Impact4[Sector Impact<br/>Healthcare ETF +2%]

    Impact2 --> Decision1{Trade Pharma?<br/>Confidence: 87%}
    Impact3 --> Decision2{Short Competitor?<br/>Confidence: 62%}
    Impact4 --> Decision3{Buy Healthcare ETF?<br/>Confidence: 71%}

    Decision1 -->|Yes| Trade1[BUY Pharma<br/>Call Options<br/>Reason: FDA approval]
    Decision2 -->|No| Skip1[Skip: Confidence too low]
    Decision3 -->|Yes| Trade2[BUY Healthcare ETF<br/>Shares<br/>Reason: Sector momentum]

    Trade1 --> Explain1[Explanation:<br/>1. FDA approval directly impacts<br/>2. High correlation with sector<br/>3. 87% confidence from ML model]
    Trade2 --> Explain2[Explanation:<br/>1. Sector-wide impact detected<br/>2. Diversified exposure<br/>3. Lower risk than single stock]

    style Decision1 fill:#faa,stroke:#333,stroke-width:2px
    style Explain1 fill:#afa,stroke:#333,stroke-width:2px
```

---

## 3. System Architecture

### 3.1 High-Level Architecture (Tier 1 - Shared Infrastructure)

```mermaid
graph TB
    subgraph "Input Sources"
        MI[Market Intelligence<br/>API Port 8000<br/>Impact Predictions]
        CORR[Correlation Tool<br/>API Port 8001<br/>Correlation Data]
        MD[Market Data<br/>Real-time Prices<br/>Options Chains]
    end

    subgraph "Trading Decision Engine (Cores 0-15, Port 8002)"
        subgraph "Signal Integration (Python)"
            SI1[Signal Aggregator]
            SI2[Feature Engineering]
            SI3[Signal Validator]
        end

        subgraph "ML Decision Models (PyTorch + CUDA)"
            ML1[Reinforcement Learning<br/>PPO Agent]
            ML2[Deep Neural Network<br/>Transformer + LSTM]
            ML3[Graph Neural Network<br/>Impact Propagation]
            ML4[Ensemble Combiner]
        end

        subgraph "Explainability Engine (Python)"
            EX1[SHAP Explainer]
            EX2[Attention Visualizer]
            EX3[Decision Tree Generator]
            EX4[Natural Language Explainer]
        end

        subgraph "Decision Components (C++23)"
            DC1[Position Sizing<br/>Kelly Criterion]
            DC2[Risk Manager<br/>Portfolio Limits]
            DC3[Order Generator<br/>Smart Routing]
        end
    end

    subgraph "Shared Resources (Same Server)"
        DB[(PostgreSQL<br/>trading_decisions DB)]
        CACHE[(Redis<br/>td:* prefix)]
        GPU[NVIDIA GPU<br/>Time-shared]
    end

    subgraph "Output & Execution"
        API[REST API<br/>Port 8002]
        WS[WebSocket<br/>Real-time Signals]
        BROKER[Broker APIs<br/>IBKR, Tastytrade]
    end

    MI & CORR & MD --> SI1 --> SI2 --> SI3
    SI3 --> ML1 & ML2 & ML3
    ML1 & ML2 & ML3 --> ML4
    ML4 --> DC1 & DC2 & DC3
    ML4 --> EX1 & EX2 & EX3 & EX4
    DC1 & DC2 & DC3 --> DB & CACHE
    DB & CACHE --> API & WS
    DC3 --> BROKER
    EX1 & EX2 & EX3 & EX4 --> API

    ML1 & ML2 & ML3 --> GPU

    style EX1 fill:#afa,stroke:#333,stroke-width:3px
    style ML4 fill:#faa,stroke:#333,stroke-width:2px
    style DB fill:#bbf,stroke:#333,stroke-width:2px
```

### 3.2 Shared Server Resource Allocation (64-Core Server)

```mermaid
graph TB
    subgraph "Single 64-Core Server - Complete Platform"
        subgraph "Market Intelligence (Cores 0-21, 22 cores)"
            MIE[MPI Ranks 0-1<br/>News Processing<br/>NLP Pipeline]
        end

        subgraph "Correlation Tool (Cores 22-42, 21 cores)"
            CAT[MPI Ranks 2-3<br/>Correlation Calc<br/>MKL/CUDA]
        end

        subgraph "Trading Decision Engine (Cores 43-63, 21 cores)"
            TDE1[RL Training<br/>PyTorch + CUDA]
            TDE2[DNN Inference<br/>Real-time]
            TDE3[Risk Manager<br/>C++23]
        end

        subgraph "Shared Resources"
            PG[("PostgreSQL<br/>3 Databases")]
            RED[("Redis<br/>3 Prefixes")]
            GPU_S["NVIDIA GPU<br/>24-48GB<br/>Time-Shared"]
        end
    end

    MIE & CAT & TDE1 --> GPU_S
    MIE & CAT & TDE1 & TDE2 & TDE3 --> PG
    MIE & CAT & TDE1 & TDE2 --> RED

    style TDE1 fill:#faa,stroke:#333,stroke-width:2px
    style PG fill:#bbf,stroke:#333,stroke-width:2px
```

**Resource Allocation (64-Core Server):**

| Component | Cores | RAM | GPU Time | Port | Database |
|-----------|-------|-----|----------|------|----------|
| Market Intelligence | 0-21 (22) | 85GB | 33% | 8000 | market_intelligence |
| Correlation Tool | 22-42 (21) | 85GB | 33% | 8001 | correlations |
| Trading Decision | 43-63 (21) | 86GB | 34% | 8002 | trading_decisions |
| **Total** | **64** | **256GB** | **100%** | - | **3 DBs, 1 instance** |

---

## 4. Component Architecture

### 4.1 Signal Integration Pipeline

```mermaid
sequenceDiagram
    participant MI as Market Intelligence API
    participant CORR as Correlation API
    participant MD as Market Data
    participant SI as Signal Integrator
    participant FE as Feature Engineer
    participant ML as ML Models
    participant EX as Explainer
    participant DM as Decision Maker

    MI->>SI: Impact predictions (HTTP)
    CORR->>SI: Correlation matrix (HTTP)
    MD->>SI: Real-time prices, options

    SI->>SI: Validate & normalize signals
    SI->>FE: Integrated signal data

    FE->>FE: Engineer features
    Note over FE: Technical indicators<br/>Sentiment scores<br/>Graph features

    FE->>ML: Feature vector
    ML->>ML: Run ensemble models
    Note over ML: RL agent<br/>DNN predictor<br/>GNN impact model

    ML->>EX: Decision + model internals
    EX->>EX: Generate explanation
    Note over EX: SHAP values<br/>Attention weights<br/>Feature importance

    ML->>DM: Trading signal + confidence
    EX->>DM: Human-readable explanation

    DM->>DM: Apply risk constraints
    DM->>DM: Calculate position size

    DM-->>User: Trading decision + explanation

    Note over DM,User: End-to-end latency: < 500ms<br/>Every decision explained
```

### 4.2 Decision Flow with Explainability

```mermaid
flowchart TB
    Start([Signal Arrives]) --> Integrate[Integrate Signals<br/>MI + Corr + Market Data]

    Integrate --> Features[Feature Engineering<br/>60+ Features]

    Features --> Ensemble[Ensemble Models]

    Ensemble --> RL[RL Agent<br/>PPO Policy]
    Ensemble --> DNN[DNN Predictor<br/>Transformer]
    Ensemble --> GNN[GNN Model<br/>Impact Graph]

    RL --> RL_Out[Action: BUY<br/>Confidence: 0.82]
    DNN --> DNN_Out[Price Target: +3.4%<br/>Probability: 0.76]
    GNN --> GNN_Out[Impact Score: 0.71<br/>Multi-hop: 3]

    RL_Out & DNN_Out & GNN_Out --> Combine[Weighted Ensemble<br/>Average + Voting]

    Combine --> Decision{Final Decision:<br/>BUY?}

    Decision -->|Yes| Explain[Generate Explanation]
    Decision -->|No| Skip[Log & Skip]

    Explain --> Why1[Why BUY?<br/>Feature Importance]
    Explain --> Why2[Why this size?<br/>Kelly calculation]
    Explain --> Why3[Risk analysis<br/>Expected drawdown]

    Why1 & Why2 & Why3 --> Readable[Human-Readable<br/>Explanation]

    Readable --> Size[Calculate Position Size<br/>Kelly Criterion]

    Size --> Risk{Pass Risk<br/>Checks?}

    Risk -->|Yes| Execute[Generate Order]
    Risk -->|No| Block[Block Trade<br/>Explain Why]

    Execute --> Log[Audit Log<br/>Full Explanation]
    Block --> Log

    Log --> End([Complete])

    style Explain fill:#faa,stroke:#333,stroke-width:3px
    style Readable fill:#afa,stroke:#333,stroke-width:2px
    style Risk fill:#f9f,stroke:#333,stroke-width:2px
```

### 4.3 Explainability Engine Architecture

```mermaid
graph TB
    subgraph "Model Outputs"
        MO1[RL Policy<br/>Action Probabilities]
        MO2[DNN Predictions<br/>Price Targets]
        MO3[GNN Embeddings<br/>Impact Scores]
        MO4[Attention Weights<br/>Feature Focus]
    end

    subgraph "Explainability Methods"
        EM1[SHAP TreeExplainer<br/>Feature Contributions]
        EM2[Integrated Gradients<br/>DNN Attribution]
        EM3[Attention Visualization<br/>What model sees]
        EM4[Counterfactual Analysis<br/>What-if scenarios]
    end

    subgraph "Explanation Generation"
        EG1[Feature Ranking<br/>Top-10 features]
        EG2[Natural Language<br/>Human description]
        EG3[Visual Charts<br/>Feature importance]
        EG4[Decision Tree<br/>Rule extraction]
    end

    subgraph "Output"
        OUT1[JSON Explanation<br/>Machine-readable]
        OUT2[Human Summary<br/>"BUY because..."]
        OUT3[Charts/Graphs<br/>Visual explanation]
        OUT4[Audit Trail<br/>Full reasoning]
    end

    MO1 --> EM1
    MO2 --> EM2
    MO3 --> EM3
    MO4 --> EM3

    EM1 & EM2 & EM3 & EM4 --> EG1 & EG2 & EG3 & EG4
    EG1 & EG2 & EG3 & EG4 --> OUT1 & OUT2 & OUT3 & OUT4

    style EM1 fill:#faa,stroke:#333,stroke-width:2px
    style OUT2 fill:#afa,stroke:#333,stroke-width:2px
```

**Explainability Example Output:**

```json
{
  "decision": {
    "action": "BUY",
    "symbol": "AAPL",
    "quantity": 100,
    "type": "CALL",
    "strike": 180,
    "expiry": "2025-11-15",
    "confidence": 0.82
  },
  "explanation": {
    "human_summary": "BUY AAPL calls because: (1) Strong earnings beat predicted by Market Intelligence (87% confidence), (2) High positive correlation with sector leaders (MSFT r=0.79), (3) Technical breakout detected, (4) Low implied volatility (opportunity)",

    "feature_importance": [
      {"feature": "mi_impact_prediction", "importance": 0.31, "value": 0.87},
      {"feature": "correlation_with_sector", "importance": 0.24, "value": 0.79},
      {"feature": "technical_breakout", "importance": 0.18, "value": 1.0},
      {"feature": "iv_percentile", "importance": 0.15, "value": 0.23},
      {"feature": "earnings_surprise", "importance": 0.12, "value": 0.15}
    ],

    "model_contributions": {
      "rl_agent": {"action": "BUY", "confidence": 0.84, "weight": 0.4},
      "dnn_predictor": {"action": "BUY", "confidence": 0.79, "weight": 0.3},
      "gnn_impact": {"action": "BUY", "confidence": 0.83, "weight": 0.3}
    },

    "risk_analysis": {
      "expected_return": 0.034,
      "expected_risk": 0.021,
      "sharpe_ratio": 1.62,
      "max_loss_scenario": -0.15,
      "probability_of_profit": 0.73
    },

    "position_sizing_rationale": {
      "method": "kelly_criterion",
      "kelly_fraction": 0.234,
      "fractional_kelly": 0.117,  // 50% of full Kelly
      "position_size_dollars": 11700,
      "reasoning": "Half-Kelly for conservative risk management"
    },

    "what_if_scenarios": [
      {"scenario": "If MI confidence was 0.5", "decision": "HOLD"},
      {"scenario": "If correlation was 0.3", "decision": "HOLD"},
      {"scenario": "If IV was 90th percentile", "decision": "Different strike"}
    ]
  }
}
```

---

## 5. Explainability & Interpretability Design

### 5.1 Multi-Level Explainability

**CRITICAL:** Not a black box! Every decision explained at multiple levels.

```mermaid
graph TB
    Decision[Trading Decision] --> L1[Level 1: Executive Summary<br/>One sentence explanation]
    Decision --> L2[Level 2: Key Factors<br/>Top 5 features with values]
    Decision --> L3[Level 3: Model Details<br/>Each model's contribution]
    Decision --> L4[Level 4: Full Analysis<br/>Complete feature set]
    Decision --> L5[Level 5: What-If Analysis<br/>Counterfactual scenarios]

    L1 --> Human1[Human: Quick Understanding]
    L2 --> Human2[Human: Decision Review]
    L3 --> Human3[Trader: Strategy Validation]
    L4 --> Human4[Analyst: Deep Dive]
    L5 --> Human5[Risk Manager: Scenario Analysis]

    style L1 fill:#afa,stroke:#333,stroke-width:2px
    style L3 fill:#faa,stroke:#333,stroke-width:2px
    style L5 fill:#bbf,stroke:#333,stroke-width:2px
```

### 5.2 SHAP (SHapley Additive exPlanations)

**Mathematical Foundation:**

```
SHAP value for feature i:
Ï†_i = Î£_{S âŠ† F\{i}} (|S|! (|F| - |S| - 1)!) / |F|! Ã— [f(S âˆª {i}) - f(S)]

Interpretation:
  Ï†_i > 0: Feature contributes positively to prediction
  Ï†_i < 0: Feature contributes negatively
  Î£ Ï†_i = prediction - baseline
```

**Implementation:**

```python
# File: src/explainability/shap_explainer.py

import shap
import numpy as np

class SHAPExplainer:
    """SHAP-based explainability for trading decisions"""

    def __init__(self, model, background_data):
        # Create SHAP explainer
        self.explainer = shap.Explainer(model, background_data)

    def explain_decision(
        self,
        features: np.ndarray,
        feature_names: list[str]
    ) -> dict:
        """Generate SHAP explanation for a decision"""

        # Compute SHAP values
        shap_values = self.explainer(features)

        # Rank features by absolute SHAP value
        feature_importance = [
            {
                'feature': feature_names[i],
                'shap_value': float(shap_values.values[0, i]),
                'feature_value': float(features[0, i]),
                'importance': abs(float(shap_values.values[0, i]))
            }
            for i in range(len(feature_names))
        ]

        # Sort by importance
        feature_importance.sort(key=lambda x: x['importance'], reverse=True)

        # Generate human-readable summary
        top_positive = [f for f in feature_importance[:5] if f['shap_value'] > 0]
        top_negative = [f for f in feature_importance[:5] if f['shap_value'] < 0]

        summary = self._generate_natural_language_summary(
            top_positive, top_negative
        )

        return {
            'feature_importance': feature_importance,
            'top_positive_factors': top_positive,
            'top_negative_factors': top_negative,
            'human_summary': summary,
            'base_value': float(shap_values.base_values[0]),
            'prediction': float(shap_values.base_values[0] + shap_values.values[0].sum())
        }

    def _generate_natural_language_summary(
        self,
        positive_factors: list,
        negative_factors: list
    ) -> str:
        """Convert SHAP values to natural language"""

        summary_parts = []

        if positive_factors:
            top_factor = positive_factors[0]
            summary_parts.append(
                f"Primary positive factor: {top_factor['feature']} "
                f"(contribution: +{top_factor['shap_value']:.3f})"
            )

        if negative_factors:
            top_factor = negative_factors[0]
            summary_parts.append(
                f"Primary negative factor: {top_factor['feature']} "
                f"(contribution: {top_factor['shap_value']:.3f})"
            )

        return '. '.join(summary_parts)
```

### 5.3 Attention Mechanism Visualization

```python
# File: src/explainability/attention_visualizer.py

import torch
import numpy as np

class AttentionVisualizer:
    """Visualize attention weights from Transformer models"""

    def explain_attention(
        self,
        model: torch.nn.Module,
        input_sequence: torch.Tensor,
        feature_names: list[str]
    ) -> dict:
        """Extract and explain attention weights"""

        # Forward pass with attention output
        with torch.no_grad():
            output, attention_weights = model(
                input_sequence,
                output_attentions=True
            )

        # Average attention across heads and layers
        avg_attention = attention_weights.mean(dim=(0, 1))  # [seq_len, seq_len]

        # Get attention to current timestep
        current_attention = avg_attention[-1, :]  # Last position

        # Rank features by attention
        feature_attention = [
            {
                'feature': feature_names[i],
                'attention_weight': float(current_attention[i]),
                'rank': i + 1
            }
            for i in range(len(feature_names))
        ]

        feature_attention.sort(
            key=lambda x: x['attention_weight'],
            reverse=True
        )

        # Generate explanation
        top_features = feature_attention[:5]
        summary = (
            f"Model focused on: {', '.join([f['feature'] for f in top_features])}. "
            f"Top feature ({top_features[0]['feature']}) received "
            f"{top_features[0]['attention_weight']:.1%} of attention."
        )

        return {
            'attention_weights': feature_attention,
            'top_attended_features': top_features,
            'human_summary': summary
        }
```

### 5.4 Decision Tree Extraction

```python
# File: src/explainability/decision_tree_extractor.py

from sklearn.tree import DecisionTreeClassifier, export_text
import numpy as np

class DecisionTreeExtractor:
    """Extract interpretable decision tree from neural network"""

    def distill_to_tree(
        self,
        model,
        training_data: np.ndarray,
        feature_names: list[str],
        max_depth: int = 5
    ) -> dict:
        """Distill neural network into interpretable decision tree"""

        # Generate predictions from neural network
        nn_predictions = model.predict(training_data)

        # Train decision tree to mimic neural network
        tree = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
        tree.fit(training_data, nn_predictions)

        # Extract rules in human-readable format
        tree_rules = export_text(tree, feature_names=feature_names)

        # Accuracy of tree approximation
        tree_predictions = tree.predict(training_data)
        fidelity = (tree_predictions == nn_predictions).mean()

        return {
            'tree_rules': tree_rules,
            'fidelity': fidelity,
            'human_readable': self._parse_tree_to_sentences(tree_rules),
            'max_depth': max_depth
        }

    def _parse_tree_to_sentences(self, tree_rules: str) -> list[str]:
        """Convert tree rules to natural language"""

        rules = []
        for line in tree_rules.split('\n'):
            if '|--- feature' in line or '|   |--- feature' in line:
                # Parse decision rule
                # Convert to: "If MI prediction > 0.7 and correlation > 0.6, then BUY"
                rules.append(self._convert_rule_to_sentence(line))

        return rules
```

---

## 6. Historical Data Sources (Low-Cost Options)

### 6.1 Free Historical Data Sources

```mermaid
graph TB
    subgraph "Free Sources (Tier 1)"
        F1[Yahoo Finance<br/>yfinance library<br/>Free, Unlimited]
        F2[Alpha Vantage<br/>Free tier<br/>500 calls/day]
        F3[Finnhub<br/>Free tier<br/>60 calls/min]
        F4[IEX Cloud<br/>Free tier<br/>50K messages/month]
        F5[Quandl/NASDAQ<br/>Free datasets<br/>EOD prices]
    end

    subgraph "Data Aggregation"
        AGG[Data Aggregator<br/>Merge sources<br/>Fill gaps]
    end

    subgraph "Storage"
        PAR[Parquet Files<br/>10 years<br/>~500GB]
        DUCK[(DuckDB<br/>Analytics)]
        PG[(PostgreSQL<br/>TimescaleDB)]
    end

    F1 & F2 & F3 & F4 & F5 --> AGG
    AGG --> PAR
    PAR --> DUCK
    DUCK --> PG

    style F1 fill:#afa,stroke:#333,stroke-width:2px
    style PAR fill:#bbf,stroke:#333,stroke-width:2px
```

**Free Historical Data Sources:**

| Source | Coverage | Frequency | Limit | Cost |
|--------|----------|-----------|-------|------|
| **Yahoo Finance** | All US stocks, 20+ years | Daily, 1-min intraday | Unlimited | Free |
| **Alpha Vantage** | Stocks, forex, crypto | Daily, intraday | 500 calls/day | Free |
| **Finnhub** | Stocks, forex | Real-time, historical | 60 calls/min | Free |
| **IEX Cloud** | US stocks | Real-time, historical | 50K msg/month | Free |
| **Quandl/NASDAQ** | EOD prices | Daily | Varies | Free tier |
| **FRED** | Economic indicators | Various | Unlimited | Free |
| **SEC EDGAR** | Company filings | As filed | Unlimited | Free |

**Data Collection Strategy:**

```python
# File: scripts/collect_historical_data_free.py
# Collect 10 years of data from free sources

import yfinance as yf
import pandas as pd
from alpha_vantage.timeseries import TimeSeries
import duckdb

def collect_all_free_data(symbols: list[str]):
    """Collect historical data from all free sources"""

    all_data = []

    # 1. Yahoo Finance (primary source - unlimited)
    print("Collecting from Yahoo Finance...")
    for symbol in symbols:
        try:
            ticker = yf.Ticker(symbol)
            hist = ticker.history(period="10y", interval="1d")
            hist['Symbol'] = symbol
            hist['Source'] = 'yahoo'
            all_data.append(hist)
        except Exception as e:
            print(f"Yahoo failed for {symbol}: {e}")

    # 2. Alpha Vantage (fill gaps, 500/day limit)
    av = TimeSeries(key='YOUR_FREE_KEY', output_format='pandas')

    for symbol in symbols[:500]:  # Respect daily limit
        try:
            data, meta = av.get_daily(symbol=symbol, outputsize='full')
            data['Symbol'] = symbol
            data['Source'] = 'alphavantage'
            all_data.append(data)
        except Exception as e:
            print(f"AlphaVantage failed for {symbol}: {e}")

    # Combine and deduplicate
    combined = pd.concat(all_data, ignore_index=True)

    # Save to Parquet (efficient storage)
    combined.to_parquet('data/historical/all_data.parquet')

    print(f"Collected data for {len(symbols)} symbols, {len(combined)} rows")
    print(f"Date range: {combined['Date'].min()} to {combined['Date'].max()}")

    return combined

# Get S&P 500 symbols (free from Wikipedia)
import pandas as pd
sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]
symbols = sp500['Symbol'].tolist()

# Collect data
data = collect_all_free_data(symbols)

# Store in DuckDB for analytics
con = duckdb.connect('data/trading.duckdb')
con.execute("CREATE TABLE historical_prices AS SELECT * FROM data")
```

**Storage Costs:**

```
10 years daily data for 5,000 securities:
- Raw CSV: ~10 GB
- Parquet compressed: ~500 MB (20x compression)
- DuckDB database: ~600 MB
- PostgreSQL (recent 2 years): ~200 MB

Total storage: ~1.3 GB (fits on any SSD)
Storage cost: $0 (use existing hardware)
```

### 6.2 Low-Cost Paid Data (If Needed)

**Tier 2 - Add Paid Data After POC Validation:**

| Provider | Data Type | Cost | Value |
|----------|-----------|------|-------|
| **Polygon.io** | Real-time + historical | $200-500/mo | Best value |
| **IEX Cloud** | Pay-per-call | ~$50-200/mo | Flexible |
| **Quandl Premium** | Specialized datasets | $50-150/mo | Niche data |
| **Tradier** | Options data | $0 (with funded acct) | Free options chains |

**Progressive Data Strategy:**
```
Month 1-2: Free data only ($0/month)
Month 3:   Add Polygon.io basic ($200/month) if POC validates
Month 4+:  Add options data if options strategies work
```

---

## 7. Charting & Visualization Tools

### 7.1 Open-Source Charting Stack

```mermaid
graph TB
    subgraph "Data Sources"
        DS1[PostgreSQL<br/>Positions & Orders]
        DS2[DuckDB<br/>Historical Analysis]
        DS3[Redis<br/>Real-time Data]
    end

    subgraph "Backend Visualization (Python)"
        VB1[Plotly<br/>Interactive Charts]
        VB2[Matplotlib<br/>Static Charts]
        VB3[Seaborn<br/>Statistical Plots]
        VB4[mplfinance<br/>Candlestick Charts]
    end

    subgraph "Frontend Visualization (JavaScript)"
        VF1[Lightweight Charts<br/>TradingView-like<br/>Free, Open-Source]
        VF2[D3.js<br/>Custom Charts<br/>Impact Graphs]
        VF3[Chart.js<br/>Simple Charts<br/>Performance Metrics]
        VF4[Cytoscape.js<br/>Network Graphs<br/>Correlation Networks]
    end

    subgraph "Dashboard (Free Tools)"
        DB1[Dash/Plotly<br/>Python Dashboard]
        DB2[Grafana<br/>Metrics Dashboard]
        DB3[Streamlit<br/>Quick Prototyping]
    end

    DS1 & DS2 & DS3 --> VB1 & VB2 & VB3 & VB4
    VB1 --> VF1 & VF2
    VB1 --> DB1
    DS1 & DS2 --> DB2
    VB1 --> DB3

    style VF1 fill:#afa,stroke:#333,stroke-width:2px
    style DB1 fill:#bbf,stroke:#333,stroke-width:2px
```

### 7.2 Real-Time Charting Solution

**Lightweight Charts by TradingView (Free, Open-Source):**

```javascript
// File: frontend/src/components/TradingChart.js
// Real-time trading chart with signals

import { createChart } from 'lightweight-charts';

class TradingChart {
    constructor(containerId) {
        this.chart = createChart(document.getElementById(containerId), {
            width: 1200,
            height: 600,
            layout: {
                background: { color: '#ffffff' },
                textColor: '#333',
            },
            grid: {
                vertLines: { color: '#e1e1e1' },
                horzLines: { color: '#e1e1e1' },
            }
        });

        this.candlestickSeries = this.chart.addCandlestickSeries();
        this.volumeSeries = this.chart.addHistogramSeries({
            color: '#26a69a',
            priceFormat: { type: 'volume' },
        });

        // Add marker series for trading signals
        this.signalMarkers = [];
    }

    updateData(priceData, volumeData) {
        this.candlestickSeries.setData(priceData);
        this.volumeSeries.setData(volumeData);
    }

    addTradingSignal(time, price, type, explanation) {
        // Add visual marker for trading decision
        this.signalMarkers.push({
            time: time,
            position: type === 'BUY' ? 'belowBar' : 'aboveBar',
            color: type === 'BUY' ? '#2196F3' : '#e91e63',
            shape: type === 'BUY' ? 'arrowUp' : 'arrowDown',
            text: type,
            size: 2
        });

        this.candlestickSeries.setMarkers(this.signalMarkers);

        // Add tooltip with explanation
        this.addExplanationTooltip(time, price, explanation);
    }

    addExplanationTooltip(time, price, explanation) {
        // Interactive tooltip showing why decision was made
        const tooltip = document.createElement('div');
        tooltip.className = 'trading-explanation';
        tooltip.innerHTML = `
            <strong>Trading Decision Explanation:</strong><br/>
            <strong>Why:</strong> ${explanation.human_summary}<br/>
            <strong>Confidence:</strong> ${(explanation.confidence * 100).toFixed(1)}%<br/>
            <strong>Key Factors:</strong>
            <ul>
                ${explanation.top_factors.map(f =>
                    `<li>${f.feature}: ${f.value.toFixed(3)}</li>`
                ).join('')}
            </ul>
        `;
        // Position tooltip at marker location
        // ... positioning logic
    }
}

// Usage
const chart = new TradingChart('trading-chart');

// WebSocket connection for real-time updates
const ws = new WebSocket('ws://localhost:8002/ws/signals');

ws.onmessage = (event) => {
    const signal = JSON.parse(event.data);

    if (signal.type === 'trading_decision') {
        chart.addTradingSignal(
            signal.timestamp,
            signal.price,
            signal.decision.action,
            signal.explanation
        );
    }
};
```

### 7.3 Decision Explanation Dashboard

```python
# File: src/visualization/decision_dashboard.py
# Dash/Plotly dashboard for decision visualization

import dash
from dash import dcc, html, Input, Output
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd

app = dash.Dash(__name__)

app.layout = html.Div([
    html.H1("Trading Decision Engine - Explainable AI Dashboard"),

    # Real-time decisions
    html.Div([
        html.H2("Recent Trading Decisions"),
        dcc.Graph(id='decisions-timeline'),
        dcc.Interval(id='interval-component', interval=5000)  # Update every 5s
    ]),

    # Feature importance
    html.Div([
        html.H2("Feature Importance (SHAP Values)"),
        dcc.Graph(id='shap-waterfall')
    ]),

    # Model contributions
    html.Div([
        html.H2("Model Contributions to Decision"),
        dcc.Graph(id='model-contributions')
    ]),

    # Performance metrics
    html.Div([
        html.H2("Strategy Performance"),
        dcc.Graph(id='performance-chart')
    ])
])

@app.callback(
    Output('decisions-timeline', 'figure'),
    Input('interval-component', 'n_intervals')
)
def update_decisions_timeline(n):
    """Show recent trading decisions with explanations"""

    # Query recent decisions from PostgreSQL
    decisions = query_recent_decisions(limit=50)

    fig = go.Figure()

    # Add candlestick chart
    fig.add_trace(go.Candlestick(
        x=decisions['timestamp'],
        open=decisions['price_open'],
        high=decisions['price_high'],
        low=decisions['price_low'],
        close=decisions['price_close'],
        name='Price'
    ))

    # Add BUY signals
    buy_signals = decisions[decisions['action'] == 'BUY']
    fig.add_trace(go.Scatter(
        x=buy_signals['timestamp'],
        y=buy_signals['price'],
        mode='markers+text',
        marker=dict(symbol='triangle-up', size=15, color='green'),
        text=buy_signals['confidence'].apply(lambda x: f"{x:.0%}"),
        textposition='top center',
        name='BUY',
        hovertemplate='<b>BUY Signal</b><br>%{customdata}<extra></extra>',
        customdata=buy_signals['explanation'].apply(lambda x: x[:100])
    ))

    # Add SELL signals
    sell_signals = decisions[decisions['action'] == 'SELL']
    fig.add_trace(go.Scatter(
        x=sell_signals['timestamp'],
        y=sell_signals['price'],
        mode='markers+text',
        marker=dict(symbol='triangle-down', size=15, color='red'),
        text=sell_signals['confidence'].apply(lambda x: f"{x:.0%}"),
        textposition='bottom center',
        name='SELL',
        hovertemplate='<b>SELL Signal</b><br>%{customdata}<extra></extra>',
        customdata=sell_signals['explanation'].apply(lambda x: x[:100])
    ))

    fig.update_layout(
        title='Trading Decisions Timeline with Explanations',
        xaxis_title='Date',
        yaxis_title='Price',
        hovermode='x unified',
        height=600
    )

    return fig

@app.callback(
    Output('shap-waterfall', 'figure'),
    Input('interval-component', 'n_intervals')
)
def update_shap_waterfall(n):
    """SHAP waterfall chart for latest decision"""

    # Get latest decision and its SHAP values
    decision = query_latest_decision()
    shap_values = decision['explanation']['feature_importance']

    # Create waterfall chart
    fig = go.Figure(go.Waterfall(
        name="Feature Contribution",
        orientation="v",
        measure=["relative"] * len(shap_values) + ["total"],
        x=[f['feature'] for f in shap_values] + ['Final Decision'],
        y=[f['shap_value'] for f in shap_values] + [sum(f['shap_value'] for f in shap_values)],
        text=[f"{f['shap_value']:.3f}" for f in shap_values] + [''],
        connector={"line": {"color": "rgb(63, 63, 63)"}},
    ))

    fig.update_layout(
        title='Decision Explanation: Feature Contributions (SHAP)',
        showlegend=True,
        height=500
    )

    return fig

if __name__ == '__main__':
    app.run_server(debug=True, host='0.0.0.0', port=8050)
```

### 7.4 Charting Tools (All Free & Open-Source)

| Tool | Purpose | Language | Cost | Features |
|------|---------|----------|------|----------|
| **Lightweight Charts** | Real-time candlestick | JavaScript | Free | TradingView-like UI |
| **Plotly/Dash** | Interactive dashboards | Python | Free | Web-based, interactive |
| **mplfinance** | Candlestick charts | Python | Free | OHLCV visualization |
| **TA-Lib** | Technical indicators | C++/Python | Free | 150+ indicators |
| **Grafana** | Metrics dashboard | Go | Free | Real-time monitoring |
| **D3.js** | Custom visualizations | JavaScript | Free | Ultimate flexibility |
| **Chart.js** | Simple charts | JavaScript | Free | Easy integration |
| **Streamlit** | Quick prototypes | Python | Free | Rapid dashboard dev |

**Total Cost for Full Charting Stack: $0**

---

## 8. Database Schema Design

### 8.1 PostgreSQL Schema - Trading Decisions

```mermaid
erDiagram
    TRADING_SIGNALS ||--o{ TRADING_DECISIONS : "generates"
    TRADING_DECISIONS ||--o{ ORDERS : "creates"
    ORDERS ||--o{ FILLS : "executed_as"
    TRADING_DECISIONS ||--o{ DECISION_EXPLANATIONS : "explained_by"
    TRADING_DECISIONS ||--o{ FEATURE_IMPORTANCE : "based_on"
    POSITIONS ||--o{ POSITION_HISTORY : "tracks"
    POSITIONS ||--o{ MARGIN_INTEREST_ACCRUAL : "accrues"
    POSITIONS ||--o{ POSITION_ADJUSTMENTS : "adjusted_by"
    CORPORATE_ACTIONS ||--o{ POSITION_ADJUSTMENTS : "triggers"

    TRADING_SIGNALS {
        uuid id PK
        timestamp signal_time
        text source
        text symbol
        text signal_type
        numeric confidence
        jsonb raw_data
    }

    TRADING_DECISIONS {
        uuid id PK
        uuid signal_id FK
        text symbol
        text action
        text asset_type
        numeric quantity
        numeric target_price
        numeric stop_loss
        numeric take_profit
        numeric confidence
        text strategy
        timestamp decided_at
        text decision_status
    }

    DECISION_EXPLANATIONS {
        uuid id PK
        uuid decision_id FK
        text human_summary
        jsonb feature_importance
        jsonb model_contributions
        jsonb risk_analysis
        jsonb what_if_scenarios
        text explainability_method
        timestamp created_at
    }

    FEATURE_IMPORTANCE {
        uuid id PK
        uuid decision_id FK
        text feature_name
        numeric shap_value
        numeric feature_value
        numeric importance_rank
    }

    ORDERS {
        uuid id PK
        uuid decision_id FK
        text symbol
        text order_type
        text side
        numeric quantity
        numeric limit_price
        text broker
        text order_status
        timestamp submitted_at
        timestamp filled_at
    }

    FILLS {
        uuid id PK
        uuid order_id FK
        numeric filled_quantity
        numeric fill_price
        numeric commission
        timestamp fill_time
    }

    POSITIONS {
        uuid id PK
        text symbol
        numeric quantity
        numeric avg_cost
        numeric current_price
        numeric unrealized_pnl
        numeric realized_pnl
        numeric total_commissions
        numeric accrued_margin_interest
        numeric total_dividends
        timestamp opened_at
        timestamp updated_at
        timestamp last_corp_action_date
    }

    POSITION_HISTORY {
        uuid id PK
        uuid position_id FK
        timestamp snapshot_time
        numeric quantity
        numeric price
        numeric pnl
        numeric margin_interest
        numeric dividends
    }

    MARGIN_INTEREST_ACCRUAL {
        uuid id PK
        uuid position_id FK
        date accrual_date
        numeric margin_balance
        numeric daily_interest_rate
        numeric daily_interest_amount
        numeric cumulative_interest
        text account_id
    }

    CORPORATE_ACTIONS {
        uuid id PK
        text symbol
        text action_type
        date announcement_date
        date ex_date
        date record_date
        date payment_date
        numeric dividend_amount
        text split_ratio
        jsonb details
    }

    POSITION_ADJUSTMENTS {
        uuid id PK
        uuid position_id FK
        uuid corporate_action_id FK
        timestamp adjustment_date
        text adjustment_type
        numeric old_quantity
        numeric new_quantity
        numeric old_cost_basis
        numeric new_cost_basis
        numeric cash_impact
        text notes
    }
```

### 8.2 TimescaleDB Tables

```sql
-- Trading signals time-series
CREATE TABLE trading_signals (
    time TIMESTAMPTZ NOT NULL,
    symbol TEXT NOT NULL,
    signal_type TEXT NOT NULL,  -- 'buy', 'sell', 'hold'
    confidence NUMERIC,
    source TEXT,  -- 'mi', 'corr', 'combined'
    PRIMARY KEY (time, symbol, signal_type)
);

SELECT create_hypertable('trading_signals', 'time');

-- Performance metrics time-series
CREATE TABLE performance_metrics (
    time TIMESTAMPTZ NOT NULL,
    strategy TEXT NOT NULL,
    portfolio_value NUMERIC,
    daily_return NUMERIC,
    sharpe_ratio NUMERIC,
    max_drawdown NUMERIC,
    win_rate NUMERIC,
    PRIMARY KEY (time, strategy)
);

SELECT create_hypertable('performance_metrics', 'time');

-- Continuous aggregate for daily performance
CREATE MATERIALIZED VIEW daily_performance
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 day', time) AS day,
    strategy,
    last(portfolio_value, time) AS portfolio_value,
    avg(daily_return) AS avg_daily_return,
    last(sharpe_ratio, time) AS sharpe_ratio
FROM performance_metrics
GROUP BY day, strategy;
```

### 8.3 DuckDB Analytical Schemas

```sql
-- Margin interest accrual tracking (CRITICAL FOR ACCURATE P&L)
CREATE TABLE margin_interest_accrual (
    id VARCHAR PRIMARY KEY,
    position_id VARCHAR REFERENCES positions(id),
    accrual_date DATE NOT NULL,
    margin_balance DOUBLE NOT NULL,  -- Amount borrowed
    daily_interest_rate DOUBLE NOT NULL,  -- Annual rate / 360
    daily_interest_amount DOUBLE NOT NULL,  -- Daily cost
    cumulative_interest DOUBLE NOT NULL,  -- Running total
    account_id VARCHAR NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_margin_accrual_date ON margin_interest_accrual(accrual_date);
CREATE INDEX idx_margin_accrual_position ON margin_interest_accrual(position_id);

-- Corporate actions tracking (MANDATORY FOR POSITION ADJUSTMENTS)
CREATE TABLE corporate_actions (
    id VARCHAR PRIMARY KEY,
    symbol VARCHAR NOT NULL,
    action_type VARCHAR NOT NULL,  -- 'dividend', 'split', 'merger', 'spinoff', 'rights_issue'
    announcement_date DATE,
    ex_date DATE,  -- CRITICAL: ex-dividend or ex-split date
    record_date DATE,
    payment_date DATE,  -- For dividends
    effective_date DATE,  -- For splits

    -- Dividend fields
    dividend_amount DOUBLE,  -- Per share
    dividend_currency VARCHAR DEFAULT 'USD',

    -- Split fields
    split_ratio VARCHAR,  -- '2:1', '1:10', etc.
    split_from INTEGER,  -- Old shares
    split_to INTEGER,  -- New shares

    -- Merger fields
    merger_acquirer VARCHAR,  -- Acquiring company
    merger_ratio DOUBLE,  -- Exchange ratio
    merger_cash_amount DOUBLE,  -- Cash component

    -- Spin-off fields
    spinoff_symbol VARCHAR,  -- New company symbol
    spinoff_ratio VARCHAR,  -- Distribution ratio

    -- Additional details
    details JSON,  -- Flexible storage for complex actions
    source VARCHAR,  -- Data source
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_corp_action_symbol ON corporate_actions(symbol);
CREATE INDEX idx_corp_action_ex_date ON corporate_actions(ex_date);
CREATE INDEX idx_corp_action_type ON corporate_actions(action_type);

-- Position adjustments due to corporate actions
CREATE TABLE position_adjustments (
    id VARCHAR PRIMARY KEY,
    position_id VARCHAR REFERENCES positions(id),
    corporate_action_id VARCHAR REFERENCES corporate_actions(id),
    adjustment_date TIMESTAMP NOT NULL,
    adjustment_type VARCHAR NOT NULL,  -- 'split', 'dividend', 'merger', 'spinoff'

    -- Quantity adjustments
    old_quantity DOUBLE NOT NULL,
    new_quantity DOUBLE NOT NULL,

    -- Cost basis adjustments
    old_cost_basis DOUBLE NOT NULL,
    new_cost_basis DOUBLE NOT NULL,

    -- Cash impact (dividends, cash mergers)
    cash_impact DOUBLE DEFAULT 0.0,  -- Positive = received, Negative = paid

    -- New positions created (spin-offs, stock mergers)
    new_position_id VARCHAR,  -- Links to new position if created

    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_position_adj_position ON position_adjustments(position_id);
CREATE INDEX idx_position_adj_corp_action ON position_adjustments(corporate_action_id);
CREATE INDEX idx_position_adj_date ON position_adjustments(adjustment_date);

-- Enhanced positions table with P&L components
CREATE TABLE positions (
    id VARCHAR PRIMARY KEY,
    account_id VARCHAR NOT NULL,
    symbol VARCHAR NOT NULL,
    quantity DOUBLE NOT NULL,
    avg_cost DOUBLE NOT NULL,  -- Average cost basis per share
    current_price DOUBLE,

    -- P&L components (COMPREHENSIVE)
    unrealized_pnl DOUBLE,  -- (Current Price - Avg Cost) Ã— Quantity
    realized_pnl DOUBLE DEFAULT 0.0,  -- From closed portions
    total_commissions DOUBLE DEFAULT 0.0,  -- Sum of all commissions
    accrued_margin_interest DOUBLE DEFAULT 0.0,  -- Cumulative interest
    total_dividends DOUBLE DEFAULT 0.0,  -- Net dividends (received - paid)

    -- Net P&L = unrealized_pnl + realized_pnl - total_commissions - accrued_margin_interest + total_dividends

    -- Corporate actions tracking
    last_corp_action_date TIMESTAMP,
    split_adjusted_cost DOUBLE,  -- Cost basis after all splits

    -- Timestamps
    opened_at TIMESTAMP NOT NULL,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    closed_at TIMESTAMP
);

CREATE INDEX idx_positions_symbol ON positions(symbol);
CREATE INDEX idx_positions_account ON positions(account_id);

-- Margin rates configuration (fetched from Schwab or manual update)
CREATE TABLE margin_rates (
    id VARCHAR PRIMARY KEY,
    broker VARCHAR NOT NULL DEFAULT 'schwab',
    tier_min DOUBLE NOT NULL,  -- Minimum balance for tier
    tier_max DOUBLE NOT NULL,  -- Maximum balance for tier
    annual_rate DOUBLE NOT NULL,  -- Annual percentage rate
    effective_date DATE NOT NULL,
    end_date DATE,  -- NULL if current
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Example Schwab rates (as of 2024)
INSERT INTO margin_rates VALUES
    ('tier1', 'schwab', 0, 24999.99, 12.825, '2024-01-01', NULL, CURRENT_TIMESTAMP),
    ('tier2', 'schwab', 25000, 49999.99, 12.325, '2024-01-01', NULL, CURRENT_TIMESTAMP),
    ('tier3', 'schwab', 50000, 99999.99, 11.825, '2024-01-01', NULL, CURRENT_TIMESTAMP);

-- Backtest results analytics
CREATE TABLE backtest_results (
    strategy_name VARCHAR,
    symbol VARCHAR,
    entry_date DATE,
    entry_price DOUBLE,
    exit_date DATE,
    exit_price DOUBLE,
    quantity INTEGER,

    -- Enhanced P&L breakdown
    gross_pnl DOUBLE,  -- Price difference only
    commissions DOUBLE,  -- Entry + exit commissions
    margin_interest DOUBLE,  -- Total interest for trade
    dividends DOUBLE,  -- Net dividends received/paid
    net_pnl DOUBLE,  -- After all costs

    return_pct DOUBLE,
    holding_period_days INTEGER,
    exit_reason VARCHAR,
    explanation JSON
);

-- Feature importance aggregation
CREATE VIEW top_features AS
SELECT
    feature_name,
    avg(abs(shap_value)) as avg_importance,
    count(*) as usage_count,
    avg(shap_value) as avg_contribution
FROM read_parquet('data/feature_importance/*.parquet')
GROUP BY feature_name
ORDER BY avg_importance DESC;

-- Decision success rate by feature
CREATE VIEW feature_success_rate AS
SELECT
    fi.feature_name,
    avg(CASE WHEN bd.pnl > 0 THEN 1.0 ELSE 0.0 END) as success_rate,
    avg(bd.pnl) as avg_pnl,
    count(*) as decision_count
FROM read_parquet('data/feature_importance/*.parquet') fi
JOIN backtest_results bd ON fi.decision_id = bd.decision_id
GROUP BY fi.feature_name
ORDER BY success_rate DESC;
```

---

## 9. API Specifications

### 9.1 Trading Decision API

```yaml
openapi: 3.0.0
info:
  title: Intelligent Trading Decision Engine API
  version: 1.0.0
  description: API for trading signals, decisions, and explanations

servers:
  - url: http://localhost:8002/api/v1

paths:
  /signals:
    get:
      summary: Get trading signals
      parameters:
        - name: symbol
          in: query
          schema:
            type: string
        - name: strategy
          in: query
          schema:
            type: string
            enum: [day_trading, short_term, long_term]
        - name: min_confidence
          in: query
          schema:
            type: number
            minimum: 0
            maximum: 1
      responses:
        '200':
          description: Trading signals with explanations
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/TradingSignals'

  /decisions/{decision_id}:
    get:
      summary: Get decision with full explanation
      parameters:
        - name: decision_id
          in: path
          required: true
          schema:
            type: string
            format: uuid
      responses:
        '200':
          description: Complete decision with explanation
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DecisionWithExplanation'

  /decisions/{decision_id}/explain:
    get:
      summary: Get detailed explanation for decision
      parameters:
        - name: decision_id
          in: path
          required: true
          schema:
            type: string
            format: uuid
        - name: level
          in: query
          schema:
            type: integer
            minimum: 1
            maximum: 5
            default: 2
          description: Explanation detail level
      responses:
        '200':
          description: Multi-level explanation
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DetailedExplanation'

  /portfolio:
    get:
      summary: Get current portfolio with analysis
      responses:
        '200':
          description: Portfolio positions and analysis
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Portfolio'

  /backtest:
    post:
      summary: Run strategy backtest
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                strategy:
                  type: string
                symbols:
                  type: array
                  items:
                    type: string
                start_date:
                  type: string
                  format: date
                end_date:
                  type: string
                  format: date
                initial_capital:
                  type: number
      responses:
        '200':
          description: Backtest results with explanations
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BacktestResults'

components:
  schemas:
    TradingSignals:
      type: object
      properties:
        signals:
          type: array
          items:
            type: object
            properties:
              symbol:
                type: string
              action:
                type: string
                enum: [BUY, SELL, HOLD]
              confidence:
                type: number
              explanation_summary:
                type: string
                description: One-sentence explanation
              recommended_position_size:
                type: number

    DecisionWithExplanation:
      type: object
      properties:
        decision:
          type: object
          properties:
            symbol:
              type: string
            action:
              type: string
            quantity:
              type: number
            confidence:
              type: number

        explanation:
          type: object
          properties:
            level_1_summary:
              type: string
              description: Executive summary (one sentence)

            level_2_key_factors:
              type: array
              items:
                type: object
                properties:
                  factor:
                    type: string
                  value:
                    type: number
                  importance:
                    type: number

            level_3_model_details:
              type: object
              properties:
                rl_agent:
                  type: object
                dnn_predictor:
                  type: object
                gnn_impact:
                  type: object

            level_4_full_analysis:
              type: object
              properties:
                all_features:
                  type: array
                shap_values:
                  type: array
                attention_weights:
                  type: array

            level_5_what_if:
              type: array
              items:
                type: object
                properties:
                  scenario:
                    type: string
                  alternative_decision:
                    type: string

    DetailedExplanation:
      type: object
      properties:
        decision_id:
          type: string
        level:
          type: integer
        content:
          type: object
        visualization_urls:
          type: array
          items:
            type: string

    Portfolio:
      type: object
      properties:
        total_value:
          type: number
        cash:
          type: number
        positions:
          type: array
          items:
            type: object
        daily_pnl:
          type: number
        total_pnl:
          type: number
        sharpe_ratio:
          type: number

    BacktestResults:
      type: object
      properties:
        strategy:
          type: string
        total_return:
          type: number
        sharpe_ratio:
          type: number
        max_drawdown:
          type: number
        win_rate:
          type: number
        trades:
          type: array
          items:
            type: object
            properties:
              entry_date:
                type: string
              exit_date:
                type: string
              pnl:
                type: number
              explanation:
                type: string
```

### 9.2 Message Format (Consistent with Other Tools)

**Same JSON + zstd compression as Market Intelligence and Correlation tools:**

```json
{
  "message_id": "td_550e8400-e29b",
  "timestamp": "2025-11-06T14:30:00.123Z",
  "type": "trading_decision",
  "version": "1.0",
  "payload": {
    "decision": {
      "symbol": "AAPL",
      "action": "BUY",
      "asset_type": "call_option",
      "strike": 180,
      "expiry": "2025-11-15",
      "quantity": 10,
      "confidence": 0.82
    },
    "explanation": {
      "summary": "BUY AAPL calls: FDA approval event (MI 87% conf), sector correlation 0.79, technical breakout, low IV",
      "top_factors": [
        {"feature": "mi_prediction", "value": 0.87, "contribution": 0.31},
        {"feature": "sector_correlation", "value": 0.79, "contribution": 0.24},
        {"feature": "technical_breakout", "value": 1.0, "contribution": 0.18}
      ],
      "models": {
        "rl_agent": {"vote": "BUY", "conf": 0.84, "weight": 0.4},
        "dnn": {"vote": "BUY", "conf": 0.79, "weight": 0.3},
        "gnn": {"vote": "BUY", "conf": 0.83, "weight": 0.3}
      }
    }
  },
  "metadata": {
    "processing_time_ms": 234,
    "models_used": ["rl_ppo", "transformer_v2", "gcn_v1"],
    "explainability_method": "shap",
    "compressed": true,
    "compression_ratio": 4.1
  }
}
```

---

## 10. Technology Stack

### 10.1 Shared Infrastructure (Same 64-Core Server)

**Complete Platform on Single Server:**

```mermaid
pie title CPU Core Allocation (64 Cores)
    "Market Intelligence (22 cores)" : 22
    "Correlation Tool (21 cores)" : 21
    "Trading Decision Engine (21 cores)" : 21
```

```mermaid
pie title GPU Time Allocation
    "Market Intelligence (NLP)" : 33
    "Correlation Tool (Matrix Ops)" : 33
    "Trading Decision (RL/DNN)" : 34
```

### 10.2 ML/RL Technology Stack

| Component | Library | Purpose | Acceleration |
|-----------|---------|---------|--------------|
| **Reinforcement Learning** ||||
| RL Framework | Stable-Baselines3 | PPO, DQN, A3C | PyTorch + CUDA |
| Environment | gym/gymnasium | Market simulation | CPU |
| Parallel Training | Ray RLlib | Distributed RL | Multi-core |
| **Deep Learning** ||||
| Framework | PyTorch 2.0+ | Neural networks | CUDA 12.3 |
| Transformers | Hugging Face | Sequence modeling | CUDA |
| GNN | PyTorch Geometric | Graph networks | CUDA |
| Training | PyTorch Lightning | Training framework | Multi-GPU |
| **Explainability** ||||
| SHAP | shap library | Feature attribution | CPU/GPU |
| Captum | PyTorch Captum | Integrated Gradients | CUDA |
| LIME | lime library | Local explanations | CPU |
| **Optimization** ||||
| Portfolio Opt | cvxpy + CVXOPT | Quadratic programming | Intel MKL |
| Position Sizing | scipy.optimize | Kelly Criterion | CPU |
| **Visualization** ||||
| Dashboards | Plotly/Dash | Interactive charts | CPU |
| Real-time Charts | Lightweight Charts | TradingView-like | Browser |
| Technical Analysis | TA-Lib | Indicators | C++/Python |

### 10.3 Mathematical Libraries

```cpp
// File: src/math/matrix_operations.hpp
// High-performance matrix operations for portfolio optimization

#include <mkl.h>
#include <Eigen/Dense>
#include <expected>

namespace trading::math {

// Portfolio optimization using Intel MKL
class PortfolioOptimizer {
public:
    // Mean-variance optimization
    auto optimize(
        const Eigen::VectorXd& expected_returns,    // From MI Engine
        const Eigen::MatrixXd& covariance_matrix,   // From Correlation Tool
        double risk_aversion,
        const Eigen::VectorXd& position_limits
    ) -> std::expected<Eigen::VectorXd, Error> {

        const int n = expected_returns.size();

        // Convert to MKL format for performance
        double* returns_ptr = const_cast<double*>(expected_returns.data());
        double* cov_ptr = const_cast<double*>(covariance_matrix.data());

        // Solve: min w^T Î£ w - Î» Î¼^T w
        // Using MKL LAPACK quadratic program solver

        std::vector<double> weights(n);

        // Call MKL solver
        // ... implementation using MKL optimization routines

        return Eigen::Map<Eigen::VectorXd>(weights.data(), n);
    }

    // Kelly Criterion for position sizing
    static double kelly_criterion(
        double win_probability,
        double avg_win,
        double avg_loss
    ) -> double {
        // Kelly formula: f* = (p * b - q) / b
        // where p = win probability, q = 1 - p
        // b = avg_win / avg_loss

        double b = avg_win / avg_loss;
        double p = win_probability;
        double q = 1.0 - p;

        double kelly = (p * b - q) / b;

        // Use fractional Kelly (50%) for safety
        return std::max(0.0, std::min(0.25, kelly * 0.5));
    }
};

} // namespace trading::math
```

---

## 11. Deployment Architecture

### 11.1 Complete Tier 1 Deployment (All Three Tools)

```mermaid
graph TB
    subgraph "64-128 Core Server - Complete BigBrotherAnalytics Platform"
        subgraph "Market Intelligence Engine"
            MI_MPI[MPI Ranks 0-1<br/>Cores 0-21<br/>Python API: 8000]
        end

        subgraph "Correlation Analysis Tool"
            CORR_MPI[MPI Ranks 2-3<br/>Cores 22-42<br/>Python API: 8001]
        end

        subgraph "Trading Decision Engine"
            TD_RL[RL Training<br/>Cores 43-52<br/>PyTorch + CUDA]
            TD_DNN[DNN Inference<br/>Cores 53-58<br/>Real-time]
            TD_EXEC[Order Execution<br/>Cores 59-63<br/>C++23]
            TD_API[Python API<br/>Port 8002]
        end

        subgraph "Shared Infrastructure"
            PG_ALL[("PostgreSQL<br/>3 Databases<br/>64GB Buffer")]
            REDIS_ALL[("Redis<br/>3 Prefixes<br/>16GB Cache")]
            GPU_ALL["NVIDIA GPU<br/>RTX 4090 24GB<br/>or A40 48GB"]
            DISK_ALL["NVMe SSD<br/>2TB<br/>Parquet Archives"]
        end

        subgraph "Monitoring & Visualization"
            PROM[Prometheus<br/>Metrics]
            GRAF[Grafana<br/>Dashboards]
            DASH[Plotly Dash<br/>Trading Dashboard<br/>Port 8050]
        end
    end

    MI_MPI & CORR_MPI & TD_RL --> GPU_ALL
    MI_MPI & CORR_MPI & TD_DNN & TD_EXEC --> PG_ALL
    MI_MPI & CORR_MPI & TD_DNN --> REDIS_ALL
    MI_MPI & CORR_MPI & TD_EXEC --> DISK_ALL

    TD_API --> DASH
    MI_MPI & CORR_MPI & TD_API -.Metrics.-> PROM
    PROM --> GRAF

    Client[Web Browser<br/>Trading Dashboard] --> DASH
    TradingEngine[Trading Client] --> TD_API

    style TD_RL fill:#faa,stroke:#333,stroke-width:3px
    style GPU_ALL fill:#f9f,stroke:#333,stroke-width:2px
    style DASH fill:#afa,stroke:#333,stroke-width:2px
```

### 11.2 Cost Analysis (Complete Platform)

**Single Server for All Three Tools:**

```
Hardware (One-Time):
  eBay 64-core server:           $2,500-6,500
  NVIDIA GPU (RTX 4090/A40):     $1,000-2,000
  2TB NVMe SSD:                  $150-300 (if not included)
  Shipping & setup:              $300-500
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total Hardware:                $3,950-9,300

Monthly Operating:
  Electricity (64-core + GPU):   $150-280
  Data subscriptions (Tier 1):   $0 (free APIs)
  RHEL (optional):               $0-30
  Internet (1Gb+):               Included
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total Monthly:                 $150-310

Software:
  All tools and libraries:       $0 (100% open-source)

Annual First Year:               $5,750-13,020

vs. Three Separate Servers:      $10,350-29,100
vs. Cloud (3x c6i.16xlarge):     $195,840/year

Savings vs Separate:             $4,400-19,800
Savings vs Cloud:                $182,000-190,000/year
```

---

## 12. Implementation Guidelines

### 12.1 Reinforcement Learning Implementation

**PPO (Proximal Policy Optimization) for Trading:**

```python
# File: src/rl/ppo_trading_agent.py
# RL agent for trading decisions with explainability

import gym
import numpy as np
import torch
import torch.nn as nn
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback

class TradingEnvironment(gym.Env):
    """Gym environment for trading (explainable)"""

    def __init__(self, historical_data, mi_predictions, correlations):
        self.data = historical_data
        self.mi_predictions = mi_predictions
        self.correlations = correlations

        # State space
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(60,),  # 60 features
            dtype=np.float32
        )

        # Action space: [hold, buy_stock, sell_stock, buy_call, buy_put]
        self.action_space = gym.spaces.Discrete(5)

        # Feature names for explainability
        self.feature_names = [
            'mi_impact_prediction', 'mi_confidence',
            'correlation_sp500', 'correlation_sector',
            'rsi_14', 'macd', 'bollinger_position',
            'volume_ratio', 'volatility_percentile',
            # ... 51 more features
        ]

    def step(self, action):
        # Execute action and return (observation, reward, done, info)

        # Calculate reward (risk-adjusted)
        reward = self._calculate_reward(action)

        # Get next state
        obs = self._get_observation()

        # Explanation info
        info = {
            'explanation': self._explain_action(action, obs),
            'feature_values': dict(zip(self.feature_names, obs))
        }

        return obs, reward, done, info

    def _explain_action(self, action, state):
        """Explain why action was taken"""

        # Get feature values that influenced decision
        feature_vals = dict(zip(self.feature_names, state))

        return {
            'action_taken': self._action_to_string(action),
            'state_features': feature_vals,
            'reasoning': self._generate_reasoning(action, feature_vals)
        }

    def _generate_reasoning(self, action, features):
        """Generate human-readable reasoning"""

        if action == 1:  # BUY
            return (
                f"BUY because: MI prediction {features['mi_impact_prediction']:.2%}, "
                f"correlation {features['correlation_sp500']:.2f}, "
                f"RSI {features['rsi_14']:.1f} (not overbought)"
            )
        # ... other actions

# Train PPO agent
class ExplainableCallback(BaseCallback):
    """Callback to log explanations during training"""

    def _on_step(self):
        # Log decision explanations
        if 'explanation' in self.locals['infos'][0]:
            explanation = self.locals['infos'][0]['explanation']
            self.logger.record('explanation/reasoning', explanation['reasoning'])

        return True

# Create and train agent
env = TradingEnvironment(historical_data, mi_predictions, correlations)
model = PPO(
    "MlpPolicy",
    env,
    verbose=1,
    tensorboard_log="./logs/",
    device='cuda'  # GPU acceleration
)

# Train with explanation logging
model.learn(
    total_timesteps=1000000,
    callback=ExplainableCallback()
)

# Save model
model.save("models/ppo_trading_agent")
```

### 12.2 Deep Neural Network with Attention

```python
# File: src/dnn/transformer_predictor.py
# Transformer-based predictor with attention for explainability

import torch
import torch.nn as nn

class ExplainableTransformerPredictor(nn.Module):
    """Transformer with attention weights for explainability"""

    def __init__(
        self,
        input_dim: int = 60,
        d_model: int = 256,
        nhead: int = 8,
        num_layers: int = 4
    ):
        super().__init__()

        # Input projection
        self.input_proj = nn.Linear(input_dim, d_model)

        # Positional encoding
        self.pos_encoder = PositionalEncoding(d_model)

        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=1024,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)

        # Output heads
        self.action_head = nn.Linear(d_model, 3)  # BUY, SELL, HOLD
        self.quantity_head = nn.Linear(d_model, 1)
        self.confidence_head = nn.Linear(d_model, 1)

        # Store attention weights for explainability
        self.attention_weights = None

    def forward(self, x, return_attention=False):
        """Forward pass with optional attention extraction"""

        # x shape: [batch, seq_len, features]

        # Project to model dimension
        x = self.input_proj(x)
        x = self.pos_encoder(x)

        # Transformer encoding (with attention capture)
        if return_attention:
            # Custom forward to capture attention
            encoded, attn_weights = self._forward_with_attention(x)
            self.attention_weights = attn_weights
        else:
            encoded = self.transformer(x)

        # Use last timestep for prediction
        last_hidden = encoded[:, -1, :]

        # Output predictions
        action_logits = self.action_head(last_hidden)
        quantity = torch.nn.functional.softplus(self.quantity_head(last_hidden))
        confidence = torch.sigmoid(self.confidence_head(last_hidden))

        outputs = {
            'action_logits': action_logits,
            'quantity': quantity,
            'confidence': confidence
        }

        if return_attention:
            outputs['attention_weights'] = self.attention_weights

        return outputs

    def explain_prediction(self, x, feature_names: list[str]):
        """Generate explanation for prediction"""

        # Forward pass with attention
        outputs = self.forward(x, return_attention=True)

        # Get attention weights (average across heads and layers)
        attn = self.attention_weights.mean(dim=(0, 1))  # [seq_len, seq_len]

        # Attention to current timestep
        current_attn = attn[-1, :]  # What features model looked at

        # Rank features by attention
        feature_attention = [
            {'feature': feature_names[i], 'attention': float(current_attn[i])}
            for i in range(len(feature_names))
        ]
        feature_attention.sort(key=lambda x: x['attention'], reverse=True)

        # Get prediction
        action = torch.argmax(outputs['action_logits'], dim=1).item()
        action_probs = torch.softmax(outputs['action_logits'], dim=1)[0]

        explanation = {
            'action': ['HOLD', 'BUY', 'SELL'][action],
            'confidence': float(outputs['confidence'][0]),
            'action_probabilities': {
                'HOLD': float(action_probs[0]),
                'BUY': float(action_probs[1]),
                'SELL': float(action_probs[2])
            },
            'attention_ranking': feature_attention[:10],  # Top 10
            'human_summary': self._generate_summary(action, feature_attention[:5])
        }

        return explanation

    def _generate_summary(self, action, top_features):
        """Generate human-readable summary"""

        action_str = ['HOLD', 'BUY', 'SELL'][action]
        features_str = ', '.join([f"{f['feature']}" for f in top_features[:3]])

        return (
            f"{action_str} decision based primarily on: {features_str}. "
            f"Model attention focused {top_features[0]['attention']:.1%} on "
            f"{top_features[0]['feature']}."
        )
```

### 12.3 Graph Neural Network for Impact

```python
# File: src/gnn/impact_gnn.py
# GNN for multi-hop impact analysis with explainability

import torch
import torch.nn as nn
from torch_geometric.nn import GCNConv, GATConv
from torch_geometric.data import Data

class ExplainableImpactGNN(nn.Module):
    """GNN for impact propagation with attention-based explainability"""

    def __init__(self, in_channels: int, hidden_channels: int = 128):
        super().__init__()

        # Graph Attention layers for explainability
        self.conv1 = GATConv(in_channels, hidden_channels, heads=4, concat=True)
        self.conv2 = GATConv(hidden_channels * 4, hidden_channels, heads=4, concat=True)
        self.conv3 = GATConv(hidden_channels * 4, hidden_channels, heads=1, concat=False)

        # Output layer
        self.classifier = nn.Linear(hidden_channels, 3)  # BUY, SELL, HOLD

        # Store attention for explainability
        self.attention_weights = []

    def forward(self, data: Data, return_attention=False):
        """Forward pass through impact graph"""

        x, edge_index = data.x, data.edge_index

        # Layer 1
        x, attn1 = self.conv1(x, edge_index, return_attention_weights=True)
        x = torch.nn.functional.relu(x)

        # Layer 2
        x, attn2 = self.conv2(x, edge_index, return_attention_weights=True)
        x = torch.nn.functional.relu(x)

        # Layer 3
        x, attn3 = self.conv3(x, edge_index, return_attention_weights=True)

        # Classification
        out = self.classifier(x)

        if return_attention:
            self.attention_weights = [attn1, attn2, attn3]
            return out, self.attention_weights
        else:
            return out

    def explain_impact_path(
        self,
        data: Data,
        target_node: int,
        node_names: dict
    ):
        """Explain impact propagation path"""

        # Forward with attention
        out, attn_weights = self.forward(data, return_attention=True)

        # Extract attention for target node
        # This shows which nodes influenced the decision
        layer3_attn = attn_weights[2]  # Last layer attention
        edge_index, edge_attn = layer3_attn

        # Find edges connected to target node
        target_edges = edge_index[1] == target_node
        source_nodes = edge_index[0][target_edges]
        attention_values = edge_attn[target_edges]

        # Rank sources by attention
        impact_path = [
            {
                'source_node': node_names[int(src)],
                'attention': float(attn),
                'impact_strength': float(attn)
            }
            for src, attn in zip(source_nodes, attention_values)
        ]

        impact_path.sort(key=lambda x: x['attention'], reverse=True)

        # Generate explanation
        summary = (
            f"Impact on {node_names[target_node]} primarily from: "
            f"{', '.join([p['source_node'] for p in impact_path[:3]])}. "
            f"Strongest influence: {impact_path[0]['source_node']} "
            f"({impact_path[0]['attention']:.1%} attention weight)"
        )

        return {
            'impact_path': impact_path,
            'multi_hop_analysis': self._trace_multi_hop(impact_path, attn_weights),
            'human_summary': summary
        }
```

---

## 13. Tier 1 POC Implementation Guide

### 13.1 POC Overview - REAL MONEY with Schwab ($30k Account)

**âš ï¸ CRITICAL DIFFERENCE FROM TYPICAL POC:**

This is NOT paper trading - this is REAL MONEY ($30,000 Schwab margin account).
- **Daily profitability is ESSENTIAL**
- **Risk management is MANDATORY**
- **Extensive backtesting BEFORE any live trade**
- **C++23 core implementation** (not Python prototypes)
- **Production-quality code from day one**

**POC Strategy:**
1. Build C++23 core engine with options valuation
2. Backtest rigorously on 10 years of data
3. Validate with walk-forward analysis
4. Simulate extensively (100K+ scenarios)
5. Deploy with conservative risk limits
6. Target $150-300/day profit (0.5-1% daily)

### 13.2 POC Timeline (8-12 Weeks, Real Money at Stake)

```mermaid
gantt
    title Trading Decision Engine POC - Real Money ($30k Schwab)
    dateFormat  YYYY-MM-DD
    section Infrastructure (Week 1-2)
    Install Tier 1 environment     :2025-11-07, 3d
    Schwab API integration         :4d
    C++23 core engine setup        :4d
    section Options Valuation (Week 3-4)
    Trinomial tree C++23           :5d
    Black-Scholes C++23            :3d
    Greeks calculation             :3d
    Portfolio pricer               :3d
    section Backtesting (Week 5-7)
    Collect 10 years data          :2d
    Implement backtest engine      :7d
    Walk-forward validation        :7d
    Optimize parameters            :7d
    section Simulation (Week 8-9)
    Monte Carlo simulator          :5d
    Risk management engine         :5d
    Daily profit validation        :5d
    section Integration (Week 10-11)
    MI + Correlation signals       :7d
    ML ensemble (RL+DNN+GNN)       :7d
    Explainability layer           :4d
    section Live Deployment (Week 12+)
    Small position testing         :7d
    Scale to full strategy         :ongoing
```

### 13.3 POC Phase 1: C++23 Core Engine (Weeks 1-4)

**Focus: Build production-quality C++23 core, NOT Python prototypes**

```cpp
// File: src/poc/core_engine.cpp
// C++23 core trading engine for Schwab POC
// Using C++23 modules for clean architecture

module;

#include <expected>
#include <vector>
#include <string>

export module trading.poc.core;

import trading.valuation.trinomial;
import trading.valuation.blackscholes;
import trading.execution.schwab_api;
import trading.risk.manager;

export namespace trading::poc {

class CoreTradingEngine {
private:
    SchwabAPIClient schwab_;
    TrinomialTreePricer trinomial_pricer_;
    BlackScholesPricer bs_pricer_;
    RiskManager risk_manager_;

public:
    CoreTradingEngine(const std::string& schwab_api_key)
        : schwab_(schwab_api_key),
          risk_manager_(30000.0)  // $30k account
    {
        // Initialize with strict risk limits
        risk_manager_.set_max_daily_loss(900.0);      // $900 max loss
        risk_manager_.set_max_position_size(1500.0);   // $1500 max position
    }

    // Main trading loop (called each market day)
    auto run_daily_trading() -> std::expected<DailyResults, Error> {

        // 1. Get signals from Market Intelligence and Correlation tools
        auto signals = fetch_signals_from_tools();

        // 2. For each signal, evaluate options
        std::vector<TradeOpportunity> opportunities;

        for (const auto& signal : signals) {
            // Get options chain from Schwab
            auto options_chain = schwab_.get_options_chain(signal.symbol);

            // Price all viable options using trinomial tree
            for (const auto& option : options_chain) {
                auto price = trinomial_pricer_.price_option(option.params);
                auto greeks = trinomial_pricer_.calculate_greeks(option.params);

                if (is_mispriced(price.value(), option.market_price, greeks.value())) {
                    opportunities.push_back({
                        .option = option,
                        .fair_value = price.value(),
                        .market_price = option.market_price,
                        .greeks = greeks.value(),
                        .signal = signal
                    });
                }
            }
        }

        // 3. Simulate all opportunities before executing
        auto validated_trades = simulate_and_validate(opportunities);

        // 4. Execute only if profitable
        DailyResults results;
        for (const auto& trade : validated_trades) {
            if (trade.expected_pnl > 50.0 &&  // Min $50 expected profit
                trade.prob_profit > 0.70) {    // Min 70% probability

                // Final risk check
                auto risk_check = risk_manager_.check_trade_risk(trade);

                if (risk_check.approved) {
                    // Execute via Schwab API
                    auto order = schwab_.submit_order(trade);
                    results.executed_trades.push_back(order);
                }
            }
        }

        return results;
    }

private:
    auto simulate_and_validate(const std::vector<TradeOpportunity>& opportunities)
        -> std::vector<ValidatedTrade> {

        // Monte Carlo simulation (10,000 scenarios)
        // Only approve trades with positive expected value
        // ...
    }
};

} // module trading.poc.core
```

**CMake Configuration for C++23 Modules:**

```cmake
# File: CMakeLists.txt
cmake_minimum_required(VERSION 4.1.2)
project(BigBrotherAnalytics_POC VERSION 1.0.0 LANGUAGES CXX)

set(CMAKE_CXX_STANDARD 23)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
set(CMAKE_CXX_EXTENSIONS OFF)

# Enable C++23 modules
set(CMAKE_CXX_SCAN_FOR_MODULES ON)

# Compiler flags
if(CMAKE_CXX_COMPILER_ID MATCHES "GNU")
    add_compile_options(
        -fmodules-ts          # Enable modules
        -std=c++23
        -O3
        -march=native
        -fopenmp              # OpenMP for parallel pricing
    )
endif()

# Find required libraries
find_package(MPI REQUIRED)
find_package(OpenMP REQUIRED)
find_package(MKL REQUIRED)
find_package(CUDAToolkit 13.0 REQUIRED)
find_package(PostgreSQL REQUIRED)
find_package(CURL REQUIRED)

# Source files (with modules)
add_executable(trading_poc
    src/poc/core_engine.cpp
    src/poc/main.cpp
    src/valuation/trinomial_tree.cpp
    src/valuation/black_scholes.cpp
    src/execution/schwab_api.cpp
    src/risk/risk_manager.cpp
)

target_link_libraries(trading_poc
    PRIVATE
        OpenMP::OpenMP_CXX
        MKL::MKL
        CUDA::cublas
        CUDA::cudart
        CURL::libcurl
        PostgreSQL::PostgreSQL
)

# C++23 module interface files
target_sources(trading_poc
    PUBLIC
        FILE_SET CXX_MODULES FILES
            src/modules/trading.valuation.trinomial.cppm
            src/modules/trading.valuation.blackscholes.cppm
            src/modules/trading.execution.schwab_api.cppm
            src/modules/trading.risk.manager.cppm
            src/modules/trading.poc.core.cppm
)
```

### 13.4 POC Phase 2: Rigorous Backtesting (Weeks 5-7)

**âš ï¸ MANDATORY BEFORE LIVE TRADING WITH $30K:**

```cpp
// File: src/poc/backtest_validator.cpp
// Rigorous backtesting before deploying real money

#include <duckdb.hpp>
#include <expected>

export module trading.poc.backtest;

import trading.poc.core;

export namespace trading::poc {

class BacktestValidator {
public:
    auto validate_strategy_comprehensive() -> std::expected<ValidationResults, Error> {

        std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
        std::cout << "CRITICAL: Validating with $30k real money at stake\n";
        std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n";

        // Level 1: Historical backtest (10 years)
        auto backtest = run_historical_backtest("2014-01-01", "2024-12-31");

        if (!backtest.has_value()) {
            return std::unexpected(backtest.error());
        }

        std::cout << "Level 1: Historical Backtest (10 years)\n";
        std::cout << "  Sharpe Ratio: " << backtest->sharpe_ratio << "\n";
        std::cout << "  Win Rate: " << backtest->win_rate << "\n";
        std::cout << "  Max Drawdown: " << backtest->max_drawdown << "\n";

        // MUST PASS criteria
        if (backtest->sharpe_ratio < 1.5 ||
            backtest->win_rate < 0.55 ||
            backtest->max_drawdown < -0.20) {

            std::cout << "\nâœ— VALIDATION FAILED - DO NOT TRADE WITH REAL MONEY\n";
            std::cout << "  Improve algorithms before deploying $30k\n";

            return std::unexpected(Error::ValidationFailed);
        }

        std::cout << "  âœ“ Historical backtest PASSED\n\n";

        // Level 2: Walk-forward validation
        auto walk_forward = run_walk_forward_validation(12, 3);  // 12mo train, 3mo test

        std::cout << "Level 2: Walk-Forward Validation\n";
        std::cout << "  Consistency Rate: " << walk_forward->consistency_rate << "\n";

        if (walk_forward->consistency_rate < 0.70) {
            std::cout << "\nâœ— VALIDATION FAILED - Strategy not consistent\n";
            return std::unexpected(Error::InconsistentStrategy);
        }

        std::cout << "  âœ“ Walk-forward validation PASSED\n\n";

        // Level 3: Monte Carlo stress testing
        auto monte_carlo = run_monte_carlo_stress_test(100000);  // 100K scenarios

        std::cout << "Level 3: Monte Carlo Stress Test (100K scenarios)\n";
        std::cout << "  Prob(Daily Profit): " << monte_carlo->prob_daily_profit << "\n";
        std::cout << "  Worst Case (1%): $" << monte_carlo->worst_case_1pct << "\n";

        if (monte_carlo->prob_daily_profit < 0.65) {
            std::cout << "\nâœ— VALIDATION FAILED - Daily profit probability too low\n";
            return std::unexpected(Error::LowWinProbability);
        }

        std::cout << "  âœ“ Monte Carlo stress test PASSED\n\n";

        // Level 4: Daily profitability check
        auto daily_sim = simulate_daily_trading(30);  // Simulate 30 days

        std::cout << "Level 4: Daily Trading Simulation (30 days)\n";
        std::cout << "  Profitable Days: " << daily_sim->profitable_days << " of 30\n";
        std::cout << "  Avg Daily P&L: $" << daily_sim->avg_daily_pnl << "\n";

        if (daily_sim->profitable_days < 20) {  // Must be profitable 2/3 of days
            std::cout << "\nâœ— VALIDATION FAILED - Not profitable enough daily\n";
            return std::unexpected(Error::InsufficientDailyProfitability);
        }

        std::cout << "  âœ“ Daily profitability simulation PASSED\n\n";

        std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
        std::cout << "âœ“ ALL VALIDATION LEVELS PASSED\n";
        std::cout << "  Strategy validated for $30k real money trading\n";
        std::cout << "  Expected daily profit: $" << daily_sim->avg_daily_pnl << "\n";
        std::cout << "  Ready to deploy with Schwab account\n";
        std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";

        return ValidationResults{
            .backtest = *backtest,
            .walk_forward = *walk_forward,
            .monte_carlo = *monte_carlo,
            .daily_simulation = *daily_sim,
            .approved = true
        };
    }

private:
    // Implement validation methods using DuckDB for fast queries
    // ...
};

} // module trading.poc.backtest
```

### 13.5 POC Phase 3: Schwab API Integration (Weeks 8-9)

**Integration with existing SchwabFirstAPI repository:**

```cpp
// File: src/execution/schwab_api_client.cpp
// Schwab API integration based on SchwabFirstAPI patterns

export module trading.execution.schwab_api;

import <string>;
import <expected>;
import <curl/curl.h>;
import <nlohmann/json.hpp>;

export namespace trading::execution {

class SchwabAPIClient {
private:
    std::string api_key_;
    std::string api_secret_;
    std::string access_token_;
    std::string account_number_;

public:
    SchwabAPIClient(
        const std::string& api_key,
        const std::string& api_secret,
        const std::string& account_number
    ) : api_key_(api_key),
        api_secret_(api_secret),
        account_number_(account_number)
    {
        // OAuth authentication (reuse patterns from SchwabFirstAPI)
        authenticate();
    }

    // Get options chain (adapted from SchwabFirstAPI)
    auto get_options_chain(const std::string& symbol)
        -> std::expected<OptionsChain, Error> {

        // Schwab API endpoint
        std::string url = "https://api.schwabapi.com/marketdata/v1/chains";
        url += "?symbol=" + symbol;
        url += "&contractType=ALL";
        url += "&includeQuotes=TRUE";

        auto response = make_authenticated_request(url);

        if (!response.has_value()) {
            return std::unexpected(response.error());
        }

        // Parse JSON response
        auto chain = parse_options_chain(response.value());

        return chain;
    }

    // Submit order (use patterns from SchwabFirstAPI)
    auto submit_options_order(
        const std::string& symbol,
        const std::string& option_symbol,
        int quantity,
        double limit_price,
        const std::string& side  // "BUY_TO_OPEN", "SELL_TO_CLOSE"
    ) -> std::expected<OrderConfirmation, Error> {

        nlohmann::json order_spec{
            {"orderType", "LIMIT"},
            {"session", "NORMAL"},
            {"duration", "DAY"},
            {"orderStrategyType", "SINGLE"},
            {"price", limit_price},
            {"orderLegCollection", nlohmann::json::array({
                {
                    {"instruction", side},
                    {"quantity", quantity},
                    {"instrument", {
                        {"symbol", option_symbol},
                        {"assetType", "OPTION"}
                    }}
                }
            })}
        };

        // Submit to Schwab
        std::string url = "https://api.schwabapi.com/trader/v1/accounts/"
                         + account_number_ + "/orders";

        auto response = make_authenticated_post(url, order_spec.dump());

        if (!response.has_value()) {
            return std::unexpected(response.error());
        }

        return parse_order_confirmation(response.value());
    }

    // Get account details (positions, buying power)
    auto get_account_details() -> std::expected<AccountInfo, Error> {

        std::string url = "https://api.schwabapi.com/trader/v1/accounts/" + account_number_;

        auto response = make_authenticated_request(url);

        if (!response.has_value()) {
            return std::unexpected(response.error());
        }

        return parse_account_info(response.value());
    }

private:
    // OAuth flow (adapt from SchwabFirstAPI)
    void authenticate() {
        // Implement Schwab OAuth 2.0 flow
        // Based on existing SchwabFirstAPI implementation
        // ...
    }

    auto make_authenticated_request(const std::string& url)
        -> std::expected<std::string, Error> {

        // CURL request with OAuth token
        // ...
    }
};

} // module trading.execution.schwab_api
```

### 13.6 POC Phase 4: Daily Profitability Validation (Weeks 10-12)

**Before deploying $30k, validate daily profitability:**

```cpp
// File: src/poc/daily_profit_validator.cpp
// Ensure daily profitability before risking real money

export module trading.poc.daily_validator;

export namespace trading::poc {

class DailyProfitValidator {
public:
    auto validate_daily_profitability(int num_days = 30)
        -> std::expected<DailyValidation, Error> {

        std::cout << "Simulating " << num_days << " days of trading...\n";
        std::cout << "Account: $30,000\n";
        std::cout << "Daily profit target: $150-300\n\n";

        std::vector<DailyResult> daily_results;
        double cumulative_pnl = 0.0;

        for (int day = 0; day < num_days; ++day) {
            // Simulate full day of trading
            auto day_result = simulate_single_day(30000.0 + cumulative_pnl);

            daily_results.push_back(day_result);
            cumulative_pnl += day_result.pnl;

            std::cout << "Day " << (day + 1) << ": "
                      << (day_result.pnl > 0 ? "+" : "")
                      << "$" << day_result.pnl
                      << " (Cumulative: $" << cumulative_pnl << ")\n";
        }

        // Analyze results
        int profitable_days = std::count_if(
            daily_results.begin(),
            daily_results.end(),
            [](const auto& r) { return r.pnl > 0; }
        );

        double avg_daily_pnl = cumulative_pnl / num_days;
        double win_rate = static_cast<double>(profitable_days) / num_days;

        std::cout << "\n";
        std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
        std::cout << "DAILY PROFITABILITY VALIDATION\n";
        std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
        std::cout << "Profitable Days: " << profitable_days << " of " << num_days
                  << " (" << (win_rate * 100) << "%)\n";
        std::cout << "Avg Daily P&L: $" << avg_daily_pnl << "\n";
        std::cout << "Total P&L: $" << cumulative_pnl << "\n";
        std::cout << "Daily Return: " << (avg_daily_pnl / 30000.0 * 100) << "%\n";

        // MUST achieve 60%+ profitable days
        if (win_rate < 0.60) {
            std::cout << "\nâœ— FAILED: Not profitable enough for $30k real money\n";
            std::cout << "  Need 60%+ profitable days, got " << (win_rate * 100) << "%\n";
            std::cout << "  DO NOT DEPLOY WITH REAL MONEY\n";
            std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";

            return std::unexpected(Error::InsufficientDailyProfitability);
        }

        // MUST average $150+/day
        if (avg_daily_pnl < 150.0) {
            std::cout << "\nâœ— FAILED: Daily profit too low for $30k account\n";
            std::cout << "  Need $150+/day average, got $" << avg_daily_pnl << "\n";
            std::cout << "  DO NOT DEPLOY WITH REAL MONEY\n";
            std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";

            return std::unexpected(Error::ProfitTargetNotMet);
        }

        std::cout << "\nâœ“ VALIDATION PASSED - Ready for $30k deployment\n";
        std::cout << "  Daily win rate: " << (win_rate * 100) << "%\n";
        std::cout << "  Expected daily profit: $" << avg_daily_pnl << "\n";
        std::cout << "  Risk: Well-managed with stop losses\n";
        std::cout << "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";

        return DailyValidation{
            .profitable_days = profitable_days,
            .total_days = num_days,
            .win_rate = win_rate,
            .avg_daily_pnl = avg_daily_pnl,
            .total_pnl = cumulative_pnl,
            .approved = true
        };
    }
};

} // module trading.poc.daily_validator
```

### 13.7 POC Success Criteria (REAL MONEY - Higher Bar)

**Before deploying $30k Schwab account:**

âœ… **MANDATORY Validation Criteria:**
- [ ] Historical backtest: Sharpe > 2.0 (higher than typical 1.5)
- [ ] Win rate: > 60% (higher than typical 55%)
- [ ] Max drawdown: < 15% (tighter than typical 20%)
- [ ] Walk-forward: > 75% consistency (higher than typical 70%)
- [ ] Daily profitability: > 60% of days profitable
- [ ] Average daily profit: > $150/day (0.5% of $30k)
- [ ] Monte Carlo: Prob(daily profit) > 65%
- [ ] Risk management: All stop losses working correctly
- [ ] Explainability: Every trade fully explained
- [ ] Schwab API: All order types working
- [ ] Real-time monitoring: Functional
- [ ] Emergency stop: Can halt all trading instantly

**Only deploy with $30k after ALL criteria met with margin of safety!**

### 13.8 POC Technology Stack (C++23 Core)

**Core Implementation (C++23 with Modules):**
- Valuation engine: C++23 (trinomial tree, Black-Scholes)
- Risk management: C++23 (microsecond latency)
- Order execution: C++23 (Schwab API integration)
- Portfolio tracking: C++23 with OpenMP
- Backtesting: C++23 with DuckDB C++ API

**ML/AI (PyTorch + CUDA):**
- RL agents: PyTorch C++ API (libtorch)
- DNN models: PyTorch with CUDA 13.0
- GNN: PyTorch Geometric
- Training: Python, inference: C++ via libtorch

**Math Libraries:**
- Intel MKL: All BLAS, LAPACK, FFT, statistics
- CUDA: cuBLAS for matrix operations
- OpenMP: Parallel pricing

**Build System:**
- CMake 4.1.2+ with C++23 modules support
- GCC 15 with -fmodules-ts
- Ninja for fast builds

**NOT a Python prototype - production C++23 from day one!**

---

### 13.2 POC Phase 1: Simple Decision Logic (Week 1)

```python
# File: poc/simple_trading_logic.py
# Week 1: Simple rule-based decisions with explanations

import duckdb
import pandas as pd
import yfinance as yf

# Collect free data
symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']
data = {}

for symbol in symbols:
    ticker = yf.Ticker(symbol)
    hist = ticker.history(period="2y")
    data[symbol] = hist

# Simple trading logic with explanation
def make_trading_decision(symbol: str, current_data: pd.DataFrame):
    """Simple trading logic with full explanation"""

    # Calculate indicators
    sma_20 = current_data['Close'].rolling(20).mean().iloc[-1]
    sma_50 = current_data['Close'].rolling(50).mean().iloc[-1]
    current_price = current_data['Close'].iloc[-1]
    rsi = calculate_rsi(current_data['Close'], 14).iloc[-1]

    # Decision logic
    decision = "HOLD"
    confidence = 0.5
    factors = []

    if current_price > sma_20 and sma_20 > sma_50:
        decision = "BUY"
        confidence += 0.2
        factors.append(f"Price ${current_price:.2f} above SMA20 ${sma_20:.2f}")
        factors.append(f"SMA20 above SMA50 (uptrend)")

    if rsi < 30:
        decision = "BUY"
        confidence += 0.15
        factors.append(f"RSI {rsi:.1f} oversold")
    elif rsi > 70:
        decision = "SELL"
        confidence += 0.15
        factors.append(f"RSI {rsi:.1f} overbought")

    # Generate explanation
    explanation = {
        'decision': decision,
        'confidence': min(confidence, 0.95),
        'summary': f"{decision} {symbol}: " + '; '.join(factors),
        'factors': factors,
        'indicators': {
            'current_price': current_price,
            'sma_20': sma_20,
            'sma_50': sma_50,
            'rsi': rsi
        }
    }

    return explanation

# Test
for symbol in symbols:
    decision = make_trading_decision(symbol, data[symbol])
    print(f"\n{symbol}:")
    print(f"  Decision: {decision['decision']}")
    print(f"  Confidence: {decision['confidence']:.1%}")
    print(f"  Explanation: {decision['summary']}")
```

**Expected Output:**
```
AAPL:
  Decision: BUY
  Confidence: 85.0%
  Explanation: BUY AAPL: Price $178.50 above SMA20 $175.23; SMA20 above SMA50 (uptrend); RSI 42.3 not overbought

âœ“ POC Week 1 Complete: Simple decisions with full explanations
```

### 13.3 POC Phase 2: Add ML Models (Week 2-3)

```python
# File: poc/ml_trading_poc.py
# Weeks 2-3: Add ML with SHAP explainability

import xgboost as xgb
import shap
import numpy as np

# Prepare features
def engineer_features(price_data, mi_predictions, correlations):
    """Feature engineering with explanation tracking"""

    features = pd.DataFrame()

    # Technical features
    features['rsi'] = calculate_rsi(price_data['Close'])
    features['macd'] = calculate_macd(price_data['Close'])
    features['bbands_position'] = calculate_bollinger_position(price_data)

    # MI features
    features['mi_prediction'] = mi_predictions['magnitude']
    features['mi_confidence'] = mi_predictions['confidence']

    # Correlation features
    features['corr_spy'] = correlations['SPY']
    features['corr_sector'] = correlations['sector_avg']

    # ... more features

    return features, list(features.columns)  # Return feature names for explainability

# Train XGBoost model
X_train, feature_names = engineer_features(train_data, mi_preds, corrs)
y_train = calculate_forward_returns(train_data, days=5)

model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    use_label_encoder=False,
    eval_metric='logloss',
    tree_method='gpu_hist'  # GPU acceleration
)

model.fit(X_train, y_train)

# Create SHAP explainer
explainer = shap.TreeExplainer(model)

# Make prediction with explanation
def predict_with_explanation(features):
    """Predict and explain"""

    # Predict
    prediction = model.predict_proba(features)[0]
    decision = ['SELL', 'HOLD', 'BUY'][np.argmax(prediction)]

    # SHAP explanation
    shap_values = explainer.shap_values(features)

    # Rank features
    feature_importance = [
        {
            'feature': feature_names[i],
            'shap_value': float(shap_values[0][i]),
            'feature_value': float(features[0][i])
        }
        for i in range(len(feature_names))
    ]
    feature_importance.sort(key=lambda x: abs(x['shap_value']), reverse=True)

    # Human summary
    top_3 = feature_importance[:3]
    summary = (
        f"{decision}: Top factors: "
        f"{top_3[0]['feature']}={top_3[0]['feature_value']:.2f}, "
        f"{top_3[1]['feature']}={top_3[1]['feature_value']:.2f}, "
        f"{top_3[2]['feature']}={top_3[2]['feature_value']:.2f}"
    )

    return {
        'decision': decision,
        'confidence': float(prediction.max()),
        'feature_importance': feature_importance,
        'human_summary': summary
    }

# Test
X_test, _ = engineer_features(test_data, test_mi_preds, test_corrs)
explanation = predict_with_explanation(X_test[:1])

print(f"Decision: {explanation['decision']}")
print(f"Confidence: {explanation['confidence']:.1%}")
print(f"Explanation: {explanation['human_summary']}")
print(f"\nTop 5 Features:")
for f in explanation['feature_importance'][:5]:
    print(f"  {f['feature']}: {f['shap_value']:+.3f} (value: {f['feature_value']:.2f})")
```

### 13.4 POC Phase 3: Visualization Dashboard (Week 4)

```python
# File: poc/trading_dashboard_poc.py
# Week 4: Interactive dashboard with Streamlit (easiest for POC)

import streamlit as st
import plotly.graph_objects as go
import pandas as pd

st.set_page_config(page_title="Trading Decision Engine POC", layout="wide")

st.title("Trading Decision Engine - Explainable AI POC")

# Sidebar for controls
st.sidebar.header("Controls")
symbol = st.sidebar.selectbox("Symbol", ['AAPL', 'MSFT', 'GOOGL', 'TSLA'])
timeframe = st.sidebar.selectbox("Timeframe", ['1D', '1W', '1M'])

# Load data
data = load_historical_data(symbol)
decisions = load_decisions(symbol)

# Main chart
st.header(f"{symbol} - Price Chart with Trading Decisions")

fig = go.Figure()

# Candlestick
fig.add_trace(go.Candlestick(
    x=data['Date'],
    open=data['Open'],
    high=data['High'],
    low=data['Low'],
    close=data['Close'],
    name='Price'
))

# Add BUY markers
buy_decisions = decisions[decisions['action'] == 'BUY']
fig.add_trace(go.Scatter(
    x=buy_decisions['date'],
    y=buy_decisions['price'],
    mode='markers+text',
    marker=dict(symbol='triangle-up', size=15, color='green'),
    text='BUY',
    name='BUY Signals'
))

st.plotly_chart(fig, use_container_width=True)

# Decision explanation section
st.header("Latest Decision Explanation")

if len(decisions) > 0:
    latest = decisions.iloc[-1]

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("Decision", latest['action'])
        st.metric("Confidence", f"{latest['confidence']:.1%}")

    with col2:
        st.metric("Expected Return", f"{latest['expected_return']:.2%}")
        st.metric("Risk (Std Dev)", f"{latest['expected_risk']:.2%}")

    with col3:
        st.metric("Sharpe Ratio", f"{latest['sharpe_ratio']:.2f}")
        st.metric("Position Size", f"${latest['position_size']:,.0f}")

    # Explanation
    st.subheader("Why this decision?")
    st.write(latest['explanation']['human_summary'])

    # Feature importance chart
    st.subheader("Feature Importance (SHAP Values)")

    features = pd.DataFrame(latest['explanation']['feature_importance'])

    fig_shap = go.Figure(go.Bar(
        x=features['shap_value'],
        y=features['feature'],
        orientation='h',
        marker=dict(
            color=features['shap_value'],
            colorscale='RdYlGn',
            cmin=-0.5,
            cmax=0.5
        )
    ))

    fig_shap.update_layout(
        title='Feature Contributions to Decision',
        xaxis_title='SHAP Value',
        height=400
    )

    st.plotly_chart(fig_shap, use_container_width=True)

# Performance metrics
st.header("Strategy Performance")

col1, col2, col3, col4 = st.columns(4)

perf = calculate_strategy_performance(decisions)

col1.metric("Total Return", f"{perf['total_return']:.2%}")
col2.metric("Win Rate", f"{perf['win_rate']:.1%}")
col3.metric("Sharpe Ratio", f"{perf['sharpe_ratio']:.2f}")
col4.metric("Max Drawdown", f"{perf['max_drawdown']:.2%}")

# Run dashboard
# streamlit run poc/trading_dashboard_poc.py
```

**Dashboard Features (All Free):**
- Real-time price charts with trading signals
- Decision explanations with SHAP values
- Feature importance visualizations
- Performance metrics
- What-if scenario analysis
- Portfolio tracking

---

## 14. Fluent Composable API Design

### 14.1 Trading Strategy Builder

```cpp
// File: src/api/strategy_builder.hpp
// Fluent API for composable trading strategies

namespace trading::api {

class StrategyBuilder {
private:
    std::string name_;
    std::vector<std::string> symbols_;
    std::string timeframe_ = "1day";
    double min_confidence_ = 0.7;
    double risk_limit_ = 0.15;
    bool use_mi_signals_ = true;
    bool use_corr_signals_ = true;
    std::vector<std::string> ml_models_;

public:
    // Fluent API methods
    auto named(std::string name) -> StrategyBuilder& {
        name_ = std::move(name);
        return *this;
    }

    auto for_symbols(std::vector<std::string> symbols) -> StrategyBuilder& {
        symbols_ = std::move(symbols);
        return *this;
    }

    auto with_timeframe(std::string tf) -> StrategyBuilder& {
        timeframe_ = std::move(tf);
        return *this;
    }

    auto require_confidence(double min_conf) -> StrategyBuilder& {
        min_confidence_ = min_conf;
        return *this;
    }

    auto limit_risk(double max_risk) -> StrategyBuilder& {
        risk_limit_ = max_risk;
        return *this;
    }

    auto use_market_intelligence() -> StrategyBuilder& {
        use_mi_signals_ = true;
        return *this;
    }

    auto use_correlations() -> StrategyBuilder& {
        use_corr_signals_ = true;
        return *this;
    }

    auto with_models(std::vector<std::string> models) -> StrategyBuilder& {
        ml_models_ = std::move(models);
        return *this;
    }

    auto build() -> std::expected<TradingStrategy, Error> {
        // Validate and build strategy
        TradingStrategy strategy{
            .name = name_,
            .symbols = symbols_,
            .timeframe = timeframe_,
            .min_confidence = min_confidence_,
            .risk_limit = risk_limit_,
            .use_mi = use_mi_signals_,
            .use_corr = use_corr_signals_,
            .models = ml_models_
        };

        return strategy;
    }
};

// Usage
auto strategy = StrategyBuilder()
    .named("Tech_Momentum")
    .for_symbols({"AAPL", "MSFT", "GOOGL", "NVDA"})
    .with_timeframe("1day")
    .require_confidence(0.75)
    .limit_risk(0.10)
    .use_market_intelligence()
    .use_correlations()
    .with_models({"rl_ppo", "transformer_v2", "gcn_impact"})
    .build();

if (strategy.has_value()) {
    auto results = execute_strategy(strategy.value());
}

} // namespace trading::api
```

---

## 14b. Options Valuation Engine (C++23 Implementation)

### 14b.1 Trinomial Tree Model (PRIMARY - American Options)

**Complete C++23 implementation with OpenMP parallelization:**

```cpp
// File: src/valuation/trinomial_tree.hpp
// Trinomial tree for American options valuation

#include <cmath>
#include <vector>
#include <algorithm>
#include <expected>
#include <mdspan>
#include <omp.h>
#include <mkl.h>

namespace valuation {

struct OptionParams {
    double spot_price;        // S - Current stock price
    double strike_price;      // K - Strike price
    double time_to_expiry;    // T - Time to expiration (years)
    double risk_free_rate;    // r - Risk-free interest rate
    double volatility;        // Ïƒ - Volatility
    double dividend_yield;    // q - Continuous dividend yield
    char option_type;         // 'C' for call, 'P' for put
    char exercise_style;      // 'A' for American, 'E' for European
};

class TrinomialTreePricer {
private:
    static constexpr int DEFAULT_STEPS = 1000;

public:
    // Price American option using trinomial tree
    auto price_option(
        const OptionParams& params,
        int num_steps = DEFAULT_STEPS
    ) -> std::expected<double, Error> {

        const double dt = params.time_to_expiry / num_steps;
        const double sqrt_3dt = std::sqrt(3.0 * dt);

        // Calculate up, middle, down factors
        const double u = std::exp(params.volatility * sqrt_3dt);
        const double m = 1.0;
        const double d = 1.0 / u;

        // Calculate risk-neutral probabilities
        const double drift = std::exp((params.risk_free_rate - params.dividend_yield) * dt / 2.0);
        const double sqrt_dt_3 = std::sqrt(dt / 3.0);
        const double up_factor = std::exp(params.volatility * sqrt_dt_3);
        const double down_factor = 1.0 / up_factor;

        const double p_u_temp = (drift - down_factor) / (up_factor - down_factor);
        const double p_u = p_u_temp * p_u_temp;

        const double p_d_temp = (up_factor - drift) / (up_factor - down_factor);
        const double p_d = p_d_temp * p_d_temp;

        const double p_m = 1.0 - p_u - p_d;

        // Validate probabilities
        if (p_u < 0 || p_d < 0 || p_m < 0) {
            return std::unexpected(Error::InvalidProbabilities);
        }

        // Build tree (2D array: [time_step][price_node])
        const int max_nodes = 2 * num_steps + 1;
        std::vector<double> option_values(max_nodes);

        // Terminal payoffs at expiration
        #pragma omp parallel for
        for (int j = -num_steps; j <= num_steps; ++j) {
            const double spot = params.spot_price * std::pow(u, j);
            const int idx = j + num_steps;

            if (params.option_type == 'C') {
                option_values[idx] = std::max(spot - params.strike_price, 0.0);
            } else {
                option_values[idx] = std::max(params.strike_price - spot, 0.0);
            }
        }

        // Backward induction through tree
        const double discount = std::exp(-params.risk_free_rate * dt);

        for (int i = num_steps - 1; i >= 0; --i) {
            const int nodes_at_step = 2 * i + 1;

            #pragma omp parallel for if(i > 100)
            for (int j = -i; j <= i; ++j) {
                const int idx = j + num_steps;

                // Continuation value (expected value from next step)
                const double continuation = discount * (
                    p_u * option_values[idx + 1] +
                    p_m * option_values[idx] +
                    p_d * option_values[idx - 1]
                );

                // Intrinsic value (immediate exercise)
                const double spot = params.spot_price * std::pow(u, j);
                double intrinsic;

                if (params.option_type == 'C') {
                    intrinsic = std::max(spot - params.strike_price, 0.0);
                } else {
                    intrinsic = std::max(params.strike_price - spot, 0.0);
                }

                // American: max(intrinsic, continuation)
                // European: continuation only
                if (params.exercise_style == 'A') {
                    option_values[idx] = std::max(intrinsic, continuation);
                } else {
                    option_values[idx] = continuation;
                }
            }
        }

        // Option value at root (current time, current price)
        return option_values[num_steps];
    }

    // Calculate Greeks using finite differences
    auto calculate_greeks(
        const OptionParams& params,
        int num_steps = DEFAULT_STEPS
    ) -> std::expected<Greeks, Error> {

        const double h_spot = params.spot_price * 0.01;  // 1% bump
        const double h_vol = 0.01;  // 1% volatility bump
        const double h_time = 1.0 / 365.0;  // 1 day

        // Base price
        auto base_price = price_option(params, num_steps);
        if (!base_price.has_value()) {
            return std::unexpected(base_price.error());
        }

        // Delta: âˆ‚V/âˆ‚S
        OptionParams params_up_spot = params;
        params_up_spot.spot_price += h_spot;
        auto price_up = price_option(params_up_spot, num_steps);

        OptionParams params_down_spot = params;
        params_down_spot.spot_price -= h_spot;
        auto price_down = price_option(params_down_spot, num_steps);

        const double delta = (price_up.value() - price_down.value()) / (2.0 * h_spot);

        // Gamma: âˆ‚Â²V/âˆ‚SÂ²
        const double gamma = (price_up.value() - 2.0 * base_price.value() + price_down.value()) / (h_spot * h_spot);

        // Theta: âˆ‚V/âˆ‚t
        OptionParams params_time = params;
        params_time.time_to_expiry -= h_time;
        auto price_time = price_option(params_time, num_steps);
        const double theta = (price_time.value() - base_price.value()) / h_time / 365.0;  // Per day

        // Vega: âˆ‚V/âˆ‚Ïƒ
        OptionParams params_vol = params;
        params_vol.volatility += h_vol;
        auto price_vol = price_option(params_vol, num_steps);
        const double vega = (price_vol.value() - base_price.value()) / h_vol / 100.0;  // Per 1% vol change

        // Rho: âˆ‚V/âˆ‚r
        OptionParams params_rate = params;
        params_rate.risk_free_rate += 0.01;  // 1% rate change
        auto price_rate = price_option(params_rate, num_steps);
        const double rho = (price_rate.value() - base_price.value()) / 0.01 / 100.0;  // Per 1% rate change

        return Greeks{
            .delta = delta,
            .gamma = gamma,
            .theta = theta,
            .vega = vega,
            .rho = rho,
            .price = base_price.value()
        };
    }
};

} // namespace valuation
```

### 14b.2 Black-Scholes Model (SECONDARY - European/Short-Term)

**C++23 implementation with Intel MKL for CDF:**

```cpp
// File: src/valuation/black_scholes.hpp
// Black-Scholes for European options (fast, analytic)

#include <cmath>
#include <mkl_vsl.h>

namespace valuation {

class BlackScholesPricer {
public:
    // Price European option (analytic formula)
    static auto price_option(const OptionParams& params)
        -> std::expected<double, Error> {

        if (params.exercise_style != 'E') {
            return std::unexpected(Error::NotEuropeanOption);
        }

        const double d1 = calculate_d1(params);
        const double d2 = d1 - params.volatility * std::sqrt(params.time_to_expiry);

        // Use Intel MKL for normal CDF (fast and accurate)
        const double N_d1 = normal_cdf_mkl(d1);
        const double N_d2 = normal_cdf_mkl(d2);
        const double N_neg_d1 = normal_cdf_mkl(-d1);
        const double N_neg_d2 = normal_cdf_mkl(-d2);

        const double discount = std::exp(-params.risk_free_rate * params.time_to_expiry);
        const double spot_discounted = params.spot_price * std::exp(-params.dividend_yield * params.time_to_expiry);

        double price;

        if (params.option_type == 'C') {
            // Call: SÂ·N(dâ‚) - KÂ·e^(-rT)Â·N(dâ‚‚)
            price = spot_discounted * N_d1 - params.strike_price * discount * N_d2;
        } else {
            // Put: KÂ·e^(-rT)Â·N(-dâ‚‚) - SÂ·N(-dâ‚)
            price = params.strike_price * discount * N_neg_d2 - spot_discounted * N_neg_d1;
        }

        return price;
    }

    // Calculate Greeks (analytic formulas - fast!)
    static auto calculate_greeks(const OptionParams& params)
        -> std::expected<Greeks, Error> {

        const double d1 = calculate_d1(params);
        const double d2 = d1 - params.volatility * std::sqrt(params.time_to_expiry);

        const double N_d1 = normal_cdf_mkl(d1);
        const double N_d2 = normal_cdf_mkl(d2);
        const double n_d1 = normal_pdf(d1);  // PDF for gamma/vega

        const double sqrt_T = std::sqrt(params.time_to_expiry);
        const double discount = std::exp(-params.risk_free_rate * params.time_to_expiry);
        const double dividend_discount = std::exp(-params.dividend_yield * params.time_to_expiry);

        // Delta
        double delta;
        if (params.option_type == 'C') {
            delta = dividend_discount * N_d1;
        } else {
            delta = dividend_discount * (N_d1 - 1.0);
        }

        // Gamma (same for call and put)
        const double gamma = (dividend_discount * n_d1) / (params.spot_price * params.volatility * sqrt_T);

        // Vega (same for call and put)
        const double vega = params.spot_price * dividend_discount * n_d1 * sqrt_T / 100.0;  // Per 1%

        // Theta
        double theta;
        const double term1 = -(params.spot_price * n_d1 * params.volatility * dividend_discount) / (2.0 * sqrt_T);
        const double term2 = params.risk_free_rate * params.strike_price * discount;
        const double term3 = params.dividend_yield * params.spot_price * dividend_discount;

        if (params.option_type == 'C') {
            theta = (term1 - term2 * N_d2 + term3 * N_d1) / 365.0;  // Per day
        } else {
            theta = (term1 + term2 * (1.0 - N_d2) - term3 * (1.0 - N_d1)) / 365.0;
        }

        // Rho
        double rho;
        if (params.option_type == 'C') {
            rho = params.strike_price * params.time_to_expiry * discount * N_d2 / 100.0;
        } else {
            rho = -params.strike_price * params.time_to_expiry * discount * (1.0 - N_d2) / 100.0;
        }

        // Calculate price
        auto price = price_option(params);

        return Greeks{
            .delta = delta,
            .gamma = gamma,
            .theta = theta,
            .vega = vega,
            .rho = rho,
            .price = price.value()
        };
    }

private:
    static double calculate_d1(const OptionParams& params) {
        const double numerator = std::log(params.spot_price / params.strike_price) +
                                (params.risk_free_rate - params.dividend_yield +
                                 0.5 * params.volatility * params.volatility) * params.time_to_expiry;
        const double denominator = params.volatility * std::sqrt(params.time_to_expiry);
        return numerator / denominator;
    }

    static double normal_cdf_mkl(double x) {
        // Use Intel MKL for fast, accurate normal CDF
        double result;
        vdCdfNorm(1, &x, &result);
        return result;
    }

    static double normal_pdf(double x) {
        // Standard normal PDF: Ï†(x) = (1/âˆš2Ï€)e^(-xÂ²/2)
        constexpr double inv_sqrt_2pi = 0.3989422804014327;
        return inv_sqrt_2pi * std::exp(-0.5 * x * x);
    }
};

} // namespace valuation
```

### 14b.3 Model Selection Logic

**When to use each model:**

```cpp
// File: src/valuation/option_pricer.hpp
// Intelligent model selection

namespace valuation {

class OptionPricer {
public:
    auto price_option_auto(const OptionParams& params)
        -> std::expected<double, Error> {

        // Decision logic for model selection
        if (params.exercise_style == 'E' && params.time_to_expiry < 30.0/365.0) {
            // European AND short-term (< 30 days): Use Black-Scholes (fastest)
            BlackScholesPricer bs;
            return bs.price_option(params);

        } else if (params.exercise_style == 'A') {
            // American: Use trinomial tree (handles early exercise)
            TrinomialTreePricer trinomial;
            return trinomial.price_option(params);

        } else {
            // European long-term: Trinomial for accuracy
            TrinomialTreePricer trinomial;
            return trinomial.price_option(params);
        }
    }

    auto calculate_greeks_auto(const OptionParams& params)
        -> std::expected<Greeks, Error> {

        // Same logic for Greeks
        if (params.exercise_style == 'E' && params.time_to_expiry < 30.0/365.0) {
            BlackScholesPricer bs;
            return bs.calculate_greeks(params);
        } else {
            TrinomialTreePricer trinomial;
            return trinomial.calculate_greeks(params);
        }
    }
};

// Usage example
auto example_option_pricing() {
    OptionParams aapl_call{
        .spot_price = 178.50,
        .strike_price = 180.00,
        .time_to_expiry = 30.0 / 365.0,  // 30 days
        .risk_free_rate = 0.05,
        .volatility = 0.25,
        .dividend_yield = 0.005,
        .option_type = 'C',
        .exercise_style = 'A'  // American
    };

    OptionPricer pricer;

    auto price = pricer.price_option_auto(aapl_call);
    if (price.has_value()) {
        std::cout << "Option price: $" << price.value() << std::endl;
    }

    auto greeks = pricer.calculate_greeks_auto(aapl_call);
    if (greeks.has_value()) {
        std::cout << "Delta: " << greeks.value().delta << std::endl;
        std::cout << "Gamma: " << greeks.value().gamma << std::endl;
        std::cout << "Theta: " << greeks.value().theta << "/day" << std::endl;
        std::cout << "Vega: " << greeks.value().vega << "/1%" << std::endl;
    }
}

} // namespace valuation
```

### 14b.4 Parallel Options Pricing (Portfolio-Level)

**Price entire options portfolio in parallel:**

```cpp
// File: src/valuation/portfolio_pricer.hpp
// Parallel pricing for multiple options

#include <vector>
#include <omp.h>

namespace valuation {

class PortfolioPricer {
public:
    // Price all options in portfolio (parallel with OpenMP)
    auto price_all_options(
        const std::vector<OptionParams>& options
    ) -> std::vector<double> {

        std::vector<double> prices(options.size());

        #pragma omp parallel for schedule(dynamic)
        for (size_t i = 0; i < options.size(); ++i) {
            OptionPricer pricer;
            auto result = pricer.price_option_auto(options[i]);

            if (result.has_value()) {
                prices[i] = result.value();
            } else {
                prices[i] = 0.0;  // Mark error
            }
        }

        return prices;
    }

    // Calculate portfolio Greeks (parallel)
    auto calculate_portfolio_greeks(
        const std::vector<OptionParams>& options,
        const std::vector<int>& positions  // +1 for long, -1 for short
    ) -> PortfolioGreeks {

        PortfolioGreeks total_greeks{};

        #pragma omp parallel for reduction(+:total_greeks.delta, total_greeks.gamma, total_greeks.theta, total_greeks.vega, total_greeks.rho)
        for (size_t i = 0; i < options.size(); ++i) {
            OptionPricer pricer;
            auto greeks = pricer.calculate_greeks_auto(options[i]);

            if (greeks.has_value()) {
                const int position = positions[i];  // +1 or -1

                total_greeks.delta += greeks.value().delta * position;
                total_greeks.gamma += greeks.value().gamma * position;
                total_greeks.theta += greeks.value().theta * position;
                total_greeks.vega += greeks.value().vega * position;
                total_greeks.rho += greeks.value().rho * position;
            }
        }

        return total_greeks;
    }
};

} // namespace valuation
```

---

## 15. C++23 Order Execution Engine (Ultra-Low Latency)

### 15.1 High-Performance Order Management

```cpp
// File: src/execution/order_manager.hpp
// C++23 ultra-low latency order execution

#include <expected>
#include <flat_map>
#include <atomic>
#include <chrono>

namespace trading::execution {

using namespace std::chrono;

// Order structure with C++23 features
struct Order {
    std::string order_id;
    std::string symbol;
    std::string side;  // "BUY" or "SELL"
    double quantity;
    double limit_price;
    std::string order_type;  // "LIMIT", "MARKET", "STOP"
    steady_clock::time_point submitted_at;
    std::string status;  // "PENDING", "FILLED", "CANCELLED"
};

// Ultra-fast order manager with std::flat_map (cache-friendly)
class OrderManager {
private:
    std::flat_map<std::string, Order> active_orders_;
    std::atomic<uint64_t> order_counter_{0};
    mutable std::shared_mutex mutex_;

public:
    // Submit order with latency tracking
    auto submit_order(
        const std::string& symbol,
        const std::string& side,
        double quantity,
        double limit_price
    ) -> std::expected<Order, Error> {

        auto start = steady_clock::now();

        // Generate order ID
        std::string order_id = generate_order_id();

        // Create order
        Order order{
            .order_id = order_id,
            .symbol = symbol,
            .side = side,
            .quantity = quantity,
            .limit_price = limit_price,
            .order_type = "LIMIT",
            .submitted_at = start,
            .status = "PENDING"
        };

        // Pre-trade risk checks (microsecond-level)
        auto risk_check = validate_risk_limits(order);
        if (!risk_check.has_value()) {
            return std::unexpected(risk_check.error());
        }

        // Store in cache-friendly flat_map
        {
            std::unique_lock lock(mutex_);
            active_orders_[order_id] = order;
        }

        // Send to broker (async, non-blocking)
        send_to_broker_async(order);

        auto end = steady_clock::now();
        auto latency = duration_cast<microseconds>(end - start).count();

        // Log ultra-low latency
        log_latency("order_submission", latency);

        return order;
    }

    // C++23 deducing this for const/non-const correctness
    template<typename Self>
    auto get_order(this Self&& self, const std::string& order_id)
        -> std::expected<Order, Error> {

        if constexpr (std::is_const_v<std::remove_reference_t<Self>>) {
            std::shared_lock lock(self.mutex_);
            auto it = self.active_orders_.find(order_id);
            if (it != self.active_orders_.end()) {
                return it->second;
            }
        } else {
            std::unique_lock lock(self.mutex_);
            auto it = self.active_orders_.find(order_id);
            if (it != self.active_orders_.end()) {
                return it->second;
            }
        }

        return std::unexpected(Error::OrderNotFound);
    }

private:
    std::string generate_order_id() {
        return "ORD_" + std::to_string(order_counter_++);
    }

    auto validate_risk_limits(const Order& order)
        -> std::expected<void, Error> {

        // Check position limits
        // Check buying power
        // Check concentration limits
        // All in microseconds

        return {};
    }

    void log_latency(const std::string& operation, int64_t latency_us) {
        // Log to Prometheus
        // Track p50, p95, p99 latencies
    }
};

} // namespace trading::execution
```

### 15.2 Risk Management Engine

```cpp
// File: src/risk/risk_manager.cpp
// Real-time risk management

#include <atomic>
#include <omp.h>

namespace trading::risk {

class RiskManager {
private:
    std::atomic<double> current_portfolio_value_{100000.0};
    std::atomic<double> daily_pnl_{0.0};
    double max_position_size_ = 0.05;  // 5% max per position
    double max_daily_loss_ = 0.03;     // 3% max daily loss

public:
    // Pre-trade risk check (microsecond-level)
    auto check_trade_risk(
        const std::string& symbol,
        double quantity,
        double price
    ) -> std::expected<RiskCheckResult, Error> {

        double position_value = quantity * price;
        double current_value = current_portfolio_value_.load(std::memory_order_relaxed);

        // Position size check
        double position_pct = position_value / current_value;
        if (position_pct > max_position_size_) {
            return std::unexpected(Error::PositionTooLarge);
        }

        // Daily loss check
        double daily_pnl = daily_pnl_.load(std::memory_order_relaxed);
        double daily_loss_pct = daily_pnl / current_value;
        if (daily_loss_pct < -max_daily_loss_) {
            return std::unexpected(Error::DailyLossLimitExceeded);
        }

        // Concentration check
        auto concentration = check_concentration(symbol, position_value);
        if (!concentration.has_value()) {
            return std::unexpected(concentration.error());
        }

        return RiskCheckResult{
            .approved = true,
            .position_size_pct = position_pct,
            .daily_loss_pct = daily_loss_pct,
            .explanation = generate_risk_explanation(position_pct, daily_loss_pct)
        };
    }

    std::string generate_risk_explanation(double pos_pct, double loss_pct) {
        return std::format(
            "Risk approved: Position {:.1f}% of portfolio (limit {:.1f}%), "
            "Daily P&L {:.2f}% (limit -{:.1f}%)",
            pos_pct * 100, max_position_size_ * 100,
            loss_pct * 100, max_daily_loss_ * 100
        );
    }
};

} // namespace trading::risk
```

---

## 16. Performance Considerations

### 16.1 Latency Targets (Complete Platform)

```mermaid
graph LR
    subgraph "Signal to Order Latency Budget"
        L1[Market Intelligence<br/>Prediction: 3-5s]
        L2[Correlation Query<br/>< 100ms]
        L3[Feature Engineering<br/>< 50ms]
        L4[ML Inference<br/>< 100ms]
        L5[Explainability<br/>< 50ms]
        L6[Risk Check<br/>< 1ms]
        L7[Order Submission<br/>< 10ms]
    end

    L1 --> L2 --> L3 --> L4 --> L5 --> L6 --> L7

    Total[Total Latency<br/>< 5.5 seconds]

    L7 --> Total

    style L6 fill:#afa,stroke:#333,stroke-width:2px
    style L7 fill:#faa,stroke:#333,stroke-width:2px
```

**Performance Targets:**

| Component | Target Latency | Technology | Cores |
|-----------|----------------|------------|-------|
| Signal Integration | < 50ms | Python async | 2 |
| Feature Engineering | < 50ms | Python 3.14+ GIL-free | 4 |
| ML Ensemble Inference | < 100ms | PyTorch CUDA | GPU |
| Explainability (SHAP) | < 50ms | Python/C++ | 2 |
| Risk Checks | < 1ms | C++23 | 1 |
| Order Submission | < 10ms | C++23 | 1 |
| **Total (Critical Path)** | **< 300ms** | **Hybrid** | **10+GPU** |

### 16.2 Throughput Benchmarks

**Trading Decisions per Second:**

| Configuration | Decisions/Sec | Latency (p95) | Explainability |
|---------------|---------------|---------------|----------------|
| Python only | 100 | 500ms | Full |
| Python + GPU | 500 | 150ms | Full |
| C++23 + GPU | 2,000 | 50ms | Limited |
| Hybrid (prod) | 1,000 | 100ms | Full |

### 16.3 Memory Usage

**Shared Server Memory (256GB Total):**

| Component | Memory | Purpose |
|-----------|--------|---------|
| PostgreSQL Buffer | 64GB | Database cache |
| Market Intelligence | 40GB | NLP models, cache |
| Correlation Tool | 40GB | Matrix operations |
| Trading Decision | 40GB | RL/DNN models |
| Redis | 16GB | Hot data cache |
| System + Monitoring | 16GB | OS, Grafana, etc |
| DuckDB Queries | 40GB | Temporary analytics |
| **Total** | **256GB** | **Fully utilized** |

---

## 17. Complete Project Structure

```
intelligent-trading-decision-engine/
â”œâ”€â”€ README.md
â”œâ”€â”€ CMakeLists.txt              # C++23 build for execution engine
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ docker-compose.trading.yml
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cpp/                    # C++23 execution engine
â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”‚   â”œâ”€â”€ main.cpp
â”‚   â”‚   â”œâ”€â”€ execution/
â”‚   â”‚   â”‚   â”œâ”€â”€ order_manager.hpp
â”‚   â”‚   â”‚   â”œâ”€â”€ order_manager.cpp
â”‚   â”‚   â”‚   â”œâ”€â”€ broker_interface.hpp
â”‚   â”‚   â”‚   â””â”€â”€ smart_router.cpp
â”‚   â”‚   â”œâ”€â”€ risk/
â”‚   â”‚   â”‚   â”œâ”€â”€ risk_manager.hpp
â”‚   â”‚   â”‚   â”œâ”€â”€ risk_manager.cpp
â”‚   â”‚   â”‚   â””â”€â”€ position_tracker.cpp
â”‚   â”‚   â””â”€â”€ math/
â”‚   â”‚       â”œâ”€â”€ portfolio_optimizer.hpp
â”‚   â”‚       â”œâ”€â”€ kelly_criterion.cpp
â”‚   â”‚       â””â”€â”€ matrix_ops.cpp
â”‚   â”‚
â”‚   â””â”€â”€ python/                 # Python ML and API
â”‚       â”œâ”€â”€ app/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ trading_api.py
â”‚       â”‚   â”œâ”€â”€ models.py
â”‚       â”‚   â””â”€â”€ schemas.py
â”‚       â”œâ”€â”€ ml/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ rl/
â”‚       â”‚   â”‚   â”œâ”€â”€ ppo_agent.py
â”‚       â”‚   â”‚   â”œâ”€â”€ dqn_agent.py
â”‚       â”‚   â”‚   â””â”€â”€ trading_env.py
â”‚       â”‚   â”œâ”€â”€ dnn/
â”‚       â”‚   â”‚   â”œâ”€â”€ transformer_predictor.py
â”‚       â”‚   â”‚   â”œâ”€â”€ lstm_predictor.py
â”‚       â”‚   â”‚   â””â”€â”€ ensemble.py
â”‚       â”‚   â””â”€â”€ gnn/
â”‚       â”‚       â”œâ”€â”€ impact_gnn.py
â”‚       â”‚       â””â”€â”€ graph_builder.py
â”‚       â”œâ”€â”€ explainability/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ shap_explainer.py
â”‚       â”‚   â”œâ”€â”€ attention_visualizer.py
â”‚       â”‚   â”œâ”€â”€ decision_tree_extractor.py
â”‚       â”‚   â””â”€â”€ natural_language.py
â”‚       â”œâ”€â”€ signals/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ signal_integrator.py
â”‚       â”‚   â”œâ”€â”€ feature_engineer.py
â”‚       â”‚   â””â”€â”€ signal_validator.py
â”‚       â”œâ”€â”€ visualization/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ decision_dashboard.py
â”‚       â”‚   â”œâ”€â”€ explainability_charts.py
â”‚       â”‚   â””â”€â”€ performance_tracker.py
â”‚       â””â”€â”€ db/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ postgres.py
â”‚           â””â”€â”€ duckdb_backtest.py
â”‚
â”œâ”€â”€ frontend/                   # Web dashboard
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ TradingChart.js
â”‚   â”‚   â”‚   â”œâ”€â”€ ExplanationPanel.js
â”‚   â”‚   â”‚   â””â”€â”€ PerformanceMetrics.js
â”‚   â”‚   â””â”€â”€ App.js
â”‚   â””â”€â”€ public/
â”‚
â”œâ”€â”€ data/                       # Data storage
â”‚   â”œâ”€â”€ historical/             # Historical data (Parquet)
â”‚   â”œâ”€â”€ models/                 # Trained models
â”‚   â”œâ”€â”€ backtests/              # Backtest results
â”‚   â””â”€â”€ explanations/           # Decision explanations
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ collect_free_data.py
â”‚   â”œâ”€â”€ train_models.py
â”‚   â”œâ”€â”€ run_backtest.py
â”‚   â””â”€â”€ verify_setup.sh
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ cpp/
â”‚   â”‚   â”œâ”€â”€ test_order_manager.cpp
â”‚   â”‚   â””â”€â”€ test_risk_manager.cpp
â”‚   â””â”€â”€ python/
â”‚       â”œâ”€â”€ test_rl_agent.py
â”‚       â”œâ”€â”€ test_explainability.py
â”‚       â””â”€â”€ test_api.py
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ trading.yaml
â”‚   â”œâ”€â”€ models.yaml
â”‚   â””â”€â”€ risk_limits.yaml
â”‚
â”œâ”€â”€ poc/                        # POC implementations
â”‚   â”œâ”€â”€ simple_trading_logic.py
â”‚   â”œâ”€â”€ ml_trading_poc.py
â”‚   â””â”€â”€ trading_dashboard_poc.py
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ api.md
    â”œâ”€â”€ strategies.md
    â””â”€â”€ explainability.md
```

---

## 18. Mathematical Deep Dive

### 18.1 Reinforcement Learning Mathematics

**Markov Decision Process (MDP) Formulation:**

```
State Space (S):
  s_t = [
    positions_t,           // Current holdings
    cash_t,                // Available capital
    prices_t,              // Current market prices
    mi_predictions_t,      // MI impact predictions
    correlations_t,        // Correlation matrix
    technicals_t,          // Technical indicators
    sentiment_t,           // Market sentiment
    risk_metrics_t         // Portfolio risk
  ] âˆˆ â„^60

Action Space (A):
  a_t âˆˆ {HOLD, BUY_STOCK, SELL_STOCK, BUY_CALL, BUY_PUT, BUY_SPREAD, ...}
  For continuous: a_t = [action_type, quantity] âˆˆ â„^2

Reward Function (R):
  r_t = Î±â‚ Â· realized_pnl_t +
        Î±â‚‚ Â· sharpe_ratio_t +
        Î±â‚ƒ Â· (-drawdown_t) +
        Î±â‚„ Â· portfolio_value_growth_t +
        penalty_t

  Where:
    Î±â‚ = 0.4  (weight for immediate profit)
    Î±â‚‚ = 0.3  (weight for risk-adjusted return)
    Î±â‚ƒ = 0.2  (weight for drawdown penalty)
    Î±â‚„ = 0.1  (weight for long-term growth)
    penalty_t = -10 if violates risk limits

Transition Dynamics:
  P(s_{t+1} | s_t, a_t) = Market dynamics (stochastic)

Value Function:
  V^Ï€(s) = ð”¼[Î£_{t=0}^âˆž Î³^t r_t | s_0 = s, Ï€]

  Where Î³ = 0.99 (discount factor for daily trading)

Policy:
  Ï€(a | s) = P(action a | state s)

Objective:
  max_Ï€ ð”¼[Î£_{t=0}^T Î³^t r_t]

  Find policy Ï€* that maximizes expected cumulative reward
```

**PPO Update Rule:**

```
Surrogate Objective:
  L^CLIP(Î¸) = ð”¼_t[min(
    r_t(Î¸) Ã‚_t,
    clip(r_t(Î¸), 1-Îµ, 1+Îµ) Ã‚_t
  )]

Where:
  r_t(Î¸) = Ï€_Î¸(a_t | s_t) / Ï€_Î¸_old(a_t | s_t)  (probability ratio)
  Ã‚_t = advantage estimate (how much better than average)
  Îµ = 0.2 (clipping parameter)

Advantage Estimation (GAE):
  Ã‚_t = Î£_{l=0}^âˆž (Î³Î»)^l Î´_{t+l}
  Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)

  Where Î» = 0.95 (GAE parameter)
```

**Explainability:** Policy gradient âˆ‡_Î¸ log Ï€_Î¸(a|s) shows which state features most influence actions.

### 18.2 Graph Neural Network Mathematics

**GNN for Multi-Hop Impact Propagation:**

```
Impact Graph:
  G = (V, E, X)
  V = nodes (companies, events)
  E = edges (relationships)
  X = node features

Node Embedding:
  h_v^(0) = x_v  (initial features)

Graph Convolution (Layer l):
  h_v^(l) = Ïƒ(W^(l) Â· AGGREGATE({h_u^(l-1) : u âˆˆ N(v)}))

  Where:
    N(v) = neighbors of node v
    AGGREGATE = mean, sum, or attention-weighted

Graph Attention (Explainable):
  Î±_{uv} = softmax_u(e_{uv})
  e_{uv} = LeakyReLU(a^T [W h_u || W h_v])

  h_v^(l) = Ïƒ(Î£_{uâˆˆN(v)} Î±_{uv} W h_u^(l-1))

  Where:
    Î±_{uv} = attention weight (explains importance of neighbor u to v)
    a, W = learnable parameters

Multi-Hop Impact:
  impact_k-hop(v) = h_v^(k)  (k layers = k hops)

Final Prediction:
  Å·_v = softmax(MLP(h_v^(K)))  (K = number of layers)

Explainability:
  - Attention weights Î±_{uv} show which neighbors matter
  - Can trace impact path through graph
  - Multi-hop aggregation reveals indirect effects
```

### 18.3 Portfolio Optimization Mathematics

**Mean-Variance Optimization (Markowitz):**

```
Given:
  Î¼ = expected returns [n Ã— 1] (from MI Engine)
  Î£ = covariance matrix [n Ã— n] (from Correlation Tool)
  w = portfolio weights [n Ã— 1]

Objective:
  min_w  (1/2) w^T Î£ w - Î» Î¼^T w

Subject to:
  Î£ w_i = 1           (fully invested)
  w_i â‰¥ 0             (long-only constraint, optional)
  w_i â‰¤ w_max         (position limits)

Lagrangian:
  L(w, Î»â‚, Î»â‚‚) = (1/2) w^T Î£ w - Î» Î¼^T w + Î»â‚(Î£w_i - 1) + Î»â‚‚(w - w_max)

KKT Conditions:
  âˆ‡_w L = Î£w - Î»Î¼ + Î»â‚1 + Î»â‚‚ = 0

Solution (unconstrained):
  w* = Î» Î£^(-1) Î¼

With constraints, solve using quadratic programming (cvxpy, MKL)
```

**Kelly Criterion for Position Sizing:**

```
Kelly Formula:
  f* = (pÂ·b - q) / b

Where:
  p = win probability (from ML model confidence)
  q = 1 - p (loss probability)
  b = average_win / average_loss (from backtests)
  f* = fraction of capital to allocate

Fractional Kelly (Conservative):
  f_used = k Â· f*

  Where k = 0.5 (use half Kelly for safety)

Example:
  p = 0.65 (65% win rate)
  b = 2.0 (avg win twice avg loss)
  q = 0.35

  f* = (0.65 Ã— 2.0 - 0.35) / 2.0 = 0.475

  f_used = 0.5 Ã— 0.475 = 0.2375 (23.75% of capital)

Explainability:
  "Position size 23.75% based on Kelly Criterion: 65% win probability,
   2:1 win/loss ratio, using conservative half-Kelly for safety"
```

---

## 19. CMake Build Configuration

```cmake
# File: CMakeLists.txt
cmake_minimum_required(VERSION 4.1.2)
project(IntelligentTradingDecisionEngine VERSION 1.0.0 LANGUAGES CXX)

set(CMAKE_CXX_STANDARD 23)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# Find packages
find_package(MPI REQUIRED)
find_package(OpenMP REQUIRED)
find_package(MKL REQUIRED)
find_package(CURL REQUIRED)
find_package(PostgreSQL REQUIRED)
find_package(nlohmann_json REQUIRED)
find_package(zstd REQUIRED)

# Performance compiler flags
add_compile_options(
    -O3
    -march=native
    -mtune=native
    -flto
    -ffast-math
    -fopenmp
)

# Source files
set(SOURCES
    src/cpp/main.cpp
    src/cpp/execution/order_manager.cpp
    src/cpp/execution/broker_interface.cpp
    src/cpp/risk/risk_manager.cpp
    src/cpp/risk/position_tracker.cpp
    src/cpp/math/portfolio_optimizer.cpp
    src/cpp/math/kelly_criterion.cpp
)

add_executable(trading_engine ${SOURCES})

target_link_libraries(trading_engine
    PRIVATE
        OpenMP::OpenMP_CXX
        MKL::MKL
        CURL::libcurl
        PostgreSQL::PostgreSQL
        nlohmann_json::nlohmann_json
        zstd::libzstd_static
)

install(TARGETS trading_engine DESTINATION bin)
```

---

## 20. Deployment Configuration

### 20.1 systemd Service File

```ini
# File: /etc/systemd/system/trading-decision-engine.service
[Unit]
Description=Intelligent Trading Decision Engine
After=network.target postgresql.service redis.service
Requires=postgresql.service redis.service

[Service]
Type=simple
User=bigbrother
Group=bigbrother
WorkingDirectory=/opt/bigbrother
Environment="PATH=/home/linuxbrew/.linuxbrew/bin:/opt/bigbrother/.venv/bin:/usr/bin"
Environment="LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib64:/home/linuxbrew/.linuxbrew/lib"
Environment="CUDA_VISIBLE_DEVICES=0"

# Run Python API server (connects to C++ execution engine via IPC)
ExecStart=/opt/bigbrother/.venv/bin/uvicorn \
    app.trading_api:app \
    --host 0.0.0.0 \
    --port 8002 \
    --workers 4

Restart=on-failure
RestartSec=5s

[Install]
WantedBy=multi-user.target
```

### 20.2 Complete Docker Compose

```yaml
# File: docker-compose.complete-platform.yml
# All three tools on one machine

version: '3.8'

services:
  # Shared PostgreSQL
  postgres:
    image: timescale/timescaledb-ha:pg16
    environment:
      POSTGRES_USER: bigbrother
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    shm_size: 64g
    deploy:
      resources:
        limits:
          memory: 64g

  # Shared Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --maxmemory 16gb
    deploy:
      resources:
        limits:
          memory: 16g

  # Market Intelligence Engine
  market_intelligence:
    build:
      context: ./market-intelligence
      dockerfile: Dockerfile
    environment:
      DATABASE_URL: postgresql://bigbrother:${DB_PASSWORD}@postgres:5432/market_intelligence
      REDIS_URL: redis://redis:6379
      REDIS_PREFIX: mi
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
    deploy:
      resources:
        limits:
          cpus: '22'
          memory: 85g
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['0']

  # Correlation Analysis Tool
  correlation_tool:
    build:
      context: ./correlation-tool
      dockerfile: Dockerfile
    environment:
      DATABASE_URL: postgresql://bigbrother:${DB_PASSWORD}@postgres:5432/correlations
      REDIS_URL: redis://redis:6379
      REDIS_PREFIX: corr
    ports:
      - "8001:8001"
    depends_on:
      - postgres
      - redis
    deploy:
      resources:
        limits:
          cpus: '21'
          memory: 85g
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['0']

  # Trading Decision Engine
  trading_engine:
    build:
      context: ./trading-engine
      dockerfile: Dockerfile
    environment:
      DATABASE_URL: postgresql://bigbrother:${DB_PASSWORD}@postgres:5432/trading_decisions
      REDIS_URL: redis://redis:6379
      REDIS_PREFIX: td
      MI_API_URL: http://market_intelligence:8000
      CORR_API_URL: http://correlation_tool:8001
    ports:
      - "8002:8002"
    depends_on:
      - postgres
      - redis
      - market_intelligence
      - correlation_tool
    deploy:
      resources:
        limits:
          cpus: '21'
          memory: 86g
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['0']

  # Trading Dashboard (Dash/Streamlit)
  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    environment:
      TRADING_API_URL: http://trading_engine:8002
    ports:
      - "8050:8050"
    depends_on:
      - trading_engine

  # Monitoring
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:
```

---

## 21. Low-Cost Historical Data Collection

### 21.1 Comprehensive Free Data Collection Script

```python
# File: scripts/collect_complete_historical_data.py
# Collect 10 years of data from multiple free sources

import yfinance as yf
import pandas as pd
import duckdb
from alpha_vantage.timeseries import TimeSeries
import requests
from fredapi import Fred
from datetime import datetime, timedelta

class FreeDataCollector:
    """Collect historical data from all free sources"""

    def __init__(self):
        self.av = TimeSeries(key='YOUR_FREE_KEY', output_format='pandas')
        self.fred = Fred(api_key='YOUR_FREE_FRED_KEY')
        self.con = duckdb.connect('data/trading.duckdb')

    def collect_all_data(self, symbols: list[str]):
        """Collect from all sources and merge"""

        print("Collecting historical data from free sources...")

        # 1. Stock prices (Yahoo Finance - primary, unlimited)
        stock_data = self._collect_yahoo(symbols)

        # 2. Options data (Tradier - free with funded account)
        # options_data = self._collect_tradier_options(symbols)

        # 3. Economic indicators (FRED - free, unlimited)
        econ_data = self._collect_fred_indicators()

        # 4. Company fundamentals (SEC EDGAR - free)
        fundamentals = self._collect_sec_data(symbols)

        # 5. News sentiment (Reddit/Twitter - free APIs)
        sentiment = self._collect_free_sentiment(symbols)

        # Merge all data sources
        complete_data = self._merge_all_sources(
            stock_data, econ_data, fundamentals, sentiment
        )

        # Store in Parquet (efficient)
        complete_data.to_parquet('data/complete_historical.parquet')

        # Also store in DuckDB for fast analytics
        self.con.execute("CREATE TABLE IF NOT EXISTS historical AS SELECT * FROM complete_data")

        return complete_data

    def _collect_yahoo(self, symbols: list[str]):
        """Collect from Yahoo Finance (unlimited, free)"""

        all_data = []

        for symbol in symbols:
            print(f"Collecting {symbol} from Yahoo Finance...")
            try:
                ticker = yf.Ticker(symbol)

                # 10 years of daily data
                hist = ticker.history(period="10y", interval="1d")

                # Add symbol column
                hist['Symbol'] = symbol

                # Calculate returns
                hist['Returns'] = hist['Close'].pct_change()
                hist['LogReturns'] = np.log(hist['Close'] / hist['Close'].shift(1))

                all_data.append(hist.reset_index())

            except Exception as e:
                print(f"  Error: {e}")

        return pd.concat(all_data, ignore_index=True)

    def _collect_fred_indicators(self):
        """Collect economic indicators from FRED (free, unlimited)"""

        indicators = {
            'GDP': 'GDP',
            'UNRATE': 'Unemployment Rate',
            'CPIAUCSL': 'CPI',
            'DFF': 'Federal Funds Rate',
            'DGS10': '10-Year Treasury',
            'VIXCLS': 'VIX'
        }

        econ_data = pd.DataFrame()

        for code, name in indicators.items():
            print(f"Collecting {name} from FRED...")
            try:
                series = self.fred.get_series(code)
                econ_data[code] = series
            except Exception as e:
                print(f"  Error: {e}")

        return econ_data.reset_index()

    def _collect_sec_data(self, symbols: list[str]):
        """Collect fundamentals from SEC EDGAR (free)"""

        # Use SEC EDGAR API (free, no key needed)
        fundamentals = []

        for symbol in symbols:
            print(f"Collecting SEC data for {symbol}...")
            try:
                # Get company CIK
                url = f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&ticker={symbol}&type=10-Q&dateb=&owner=exclude&count=100&output=json"

                response = requests.get(url, headers={'User-Agent': 'MyApp/1.0'})

                if response.status_code == 200:
                    # Parse filings
                    # ... extract fundamental data
                    pass

            except Exception as e:
                print(f"  Error: {e}")

        return pd.DataFrame(fundamentals)

# Usage
collector = FreeDataCollector()

# Get S&P 500 symbols (free from Wikipedia)
sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]
symbols = sp500['Symbol'].tolist()

# Collect all data
complete_data = collector.collect_all_data(symbols)

print(f"\nData collection complete!")
print(f"Total rows: {len(complete_data):,}")
print(f"Date range: {complete_data['Date'].min()} to {complete_data['Date'].max()}")
print(f"Storage: Parquet file ~500MB, DuckDB ~600MB")
```

**Expected Data Collection:**
```
Source                  Data Type               Cost      Time
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Yahoo Finance          10 years daily (500 stocks)  $0      2 hours
FRED                   Economic indicators          $0      10 mins
SEC EDGAR              Quarterly fundamentals       $0      30 mins
Reddit/Twitter         Sentiment (via APIs)         $0      20 mins
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total                  Complete dataset             $0      3 hours
```

---

## 22. Performance Benchmarks & Optimization

### 22.1 End-to-End Performance

**Complete Decision Pipeline (64-Core Server + GPU):**

| Stage | Time | Technology | Cores/GPU |
|-------|------|------------|-----------|
| Fetch MI Prediction | 50ms | HTTP + Redis cache | - |
| Fetch Correlations | 20ms | PostgreSQL query | - |
| Fetch Market Data | 30ms | Market data feed | - |
| Feature Engineering | 40ms | Python 3.14+ GIL-free | 4 |
| ML Inference (Ensemble) | 80ms | PyTorch CUDA | GPU |
| SHAP Explainability | 45ms | Python/CUDA | GPU |
| Risk Management | 0.5ms | C++23 | 1 |
| Order Generation | 5ms | C++23 | 1 |
| **Total (Critical Path)** | **270ms** | **Hybrid** | **6+GPU** |

Target: < 500ms âœ“ (achieved with 270ms)

### 22.2 Scalability

**Decisions per Second (Production Load):**

- Sequential: 3-4 decisions/sec
- Parallel (batch): 50-100 decisions/sec
- GPU batch inference: 200-500 decisions/sec

**Daily Capacity:**
- Conservative: 10,000 decisions/day
- Peak: 50,000+ decisions/day

---

## Change Log

| Version | Date | Changes |
|---------|------|---------|
| 1.0.0 | 2025-11-06 | Initial architecture design for Intelligent Trading Decision Engine |

---

## References

1. [BigBrotherAnalytics PRD](../PRD.md)
2. [Market Intelligence Engine Architecture](./market-intelligence-engine.md)
3. [Correlation Analysis Tool Architecture](./trading-correlation-analysis-tool.md)
4. [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
5. [SHAP Documentation](https://shap.readthedocs.io/)
6. [Lightweight Charts](https://tradingview.github.io/lightweight-charts/)
7. [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/)

---

**Document Status:** Ready for POC implementation
**Next Steps:** Begin 4-6 week POC with free data and simple models
**Deployment:** Shares 64-core server with other two tools (Cores 43-63)
**Cost:** $0 for POC, leverages existing infrastructure
